\chapter{State of the Art of Formal Verification}
\label{chapter:relatedwork}

\endquote{``\emph{Walking on water and developing software from a specification
    are easy if both are frozen.}''

  \hfill\footnotesize --- Edward V. Berard}

\vspace{1cm}\noindent
%
Our interest in formally verify hardware architecture in order to improve their
security is in line with an ongoing effort.
%
In 2011, Nachiketh Potlapally\,\cite{potlapally2011hardwaresecurity}, who was
working at Intel at the time, suggested that formal methods for security
validation is one of the possible solutions to uncover architectural attacks
ahead of time.
%
In 2016, Stephen Chong \emph{et al.} cite ``Hardware Architectures'' as area of
interest regarding the use of formal methods for
security\,\cite{chong2016report}.

The rest of this Chapter proceeds as follows.
%
First, we give primer on formal verification of hardware or software systems,
with a focus on security (Section~\ref{sec:related:review}).
%
Then, we focus on design methods which can be leveraged to ease the verification
process, in particular in regards to the model scale
(Section~\ref{sec:related:ease}).

\section{Formal Verification for Security}
\label{sec:related:review}

Formal verification consists in rigorously proving the correctness of a system
with respect to certain properties.
%
It is achieved \emph{via} the use of formal methods to exhibit a proof on an
abstract model (\ref{subsec:state:reason}).
%
Therefore, the formal definition of targeted security properties against the
model is therefore an important step of the process (\ref{subsec:state:secu}).
%
Many approaches have been proposed for formally verifying systems of varying
sizes and natures (\ref{subsec:state:approaches}).

\subsection{Reasoning about Systems}
\label{subsec:state:reason}

We are interested in hardware or software systems primarily identified by their
mutable state.
%
Modelling such a system $C$ is usually done by defining a transition system
$M = \langle S, \rightarrow \rangle$, where $S$ is a set of abstract states
which describe the concrete states of $C$ and $\rightarrow$ is a transition
relation, such that for $(p, q) \in S \times S$, $p \rightarrow q$ means it is
possible for the system whose state corresponds to $p$ to shift in a new state
which corresponds to $q$.

The structure of $M$ is given as an example, the definition of the transition
relation $\rightarrow$ may vary from one formalism to another.
%
In particular, systems often interact with an environment, and the model
definition includes a notion of interface, that is a set of operations exposed
by the system for its environment to use.
%
The interface of a \ac{cpu} is its instructions set, the interface of a system
software is its system calls, etc.

\subsection{Specifying Security Properties}
\label{subsec:state:secu}

Once a system $C$ is modelled with a model $M$, we can reason about the
correctness of $C$ with respect to certain properties, by formalizing these
properties as a statement about $M$, then we exhibit a proof that this statement
is true.
%
This statement is based on the system executions, modelled as sequences of
transitions of $M$.
%
We denote $\Sigma(M)$ the set of all the possible executions of $C$, as modelled
in $M$.

\paragraph{Properties and Hyperproperties}
%
Certain security properties can be modelled \emph{via} a predicate $P$ on
executions.
%
In such a condition, the statement describing the security property is of the
forms
%
\[
  \forall \rho \in \Sigma(M), P(\rho)
\]
%
It is the case of safety properties (nothing ``bad'' happens) and liveness
properties (something ``good'' eventually happens).
%
For safety properties, a common approach is to distinguish between secure states
and insecure states, secure transitions and insecure transitions, and to require
that for an execution which starts from a secure state put the system in an
insecure state and no insecure transition occurs.
%
For instance, the BIOS Integrity security property
(Definition~\ref{def:usecase:biosint}) is a safety property security property.
%
As for liveness properties, they usually require to use a temporal logic which
provides a modal operator to reason about ``eventually''.
%
For instance, the BIOS Availability security property
(Definition~\ref{def:usecase:biosav}) is a liveness property

Other security (hyper)properties cannot be defined using such a predicate, and
requires to consider the set of executions as a whole. As a consequence, the
security statement is of the form
executions\,\cite{clarkson2010hyperproperties}, that is
%
\[
  P(\Sigma(M))
\]
%
A notable security property whose definition requires to consider couple of
executions is noninterference\,\cite{goguen1982security} of information flow
analysis.
%
A system enforces noninterference if its observable behaviour does not depends
on secret inputs, which informally translates to ``a system enforces
noninterference if, given two arbitrary executions with the same public inputs,
the system's observable behaviour remains the same''.

\paragraph{Modelling for Security}
%
The definition of a security property might be based on information that are not
available to the system $C$.
%
For instance, a x86 hardware architecture does not track information flows, but
it is supposed to enforce the BIOS Integrity security property, whose definition
requires to be able to tell which software components has been the source of the
result of a read access by the SMM code.
%
As a consequence, if we want to verify the x86 hardware architecture correctness
with respect to the BIOS Integrity security property, we need a ``information
flow aware'' model.

This raises questions about model reusability.
%
The x86 hardware architecture includes many different \ac{hse} mechanisms.
%
Defining one model for each is not an option, yet the security properties they
aim to enforce may heavily differ in terms of model requirements.

\subsection{Verification Approaches}
\label{subsec:state:approaches}

The last step of the formal verification is exhibiting a proof.
%
There are two main approaches to achieve this: model checking and theorem
proving.
%
For each approach, we give a small overview of its core principles.
%
In addition, we illustrate their verification process \emph{via} a common,
minimal example: an airlock system.
%
A airlock system is a device made of two doors, and an intermediary chamber.
%
To pass through the airlock system, it is necessary to open the first door,
enter the chamber, wait for the first door to close, then open the second door.
%
The security property of a airlock system is a safety property: at any time,
there is at least one door closed.

\paragraph{Model Checking}
%
Model checkers exhaustively and automatically explore a transition system in
order to verify it satisfies a specified properties.
%
These properties are defined it terms of predicate on the system's executions,
usually thanks to a temporal logic.
%
A temporal logic includes a set of modal operators, such as \emph{globally} (the
property is always true), or \emph{eventually} (the property becomes true in at
one point in the future).
%
The main advantage of model checkers is automation.
%
They take a model and a property as input, and output a result: a confirmation
that the property is verified, or a particular execution which does not satisfy
the property.
%
This execution is called a counterexample.
%
From a security perspective, a counterexample is very interesting, because it
potentially describes an attack path.

\begin{example}[Airlock System Verification in NuSMV]
  TODO.

  \inputminted[frame=single]{coq}{Listings/Airlock1.nusmv}
  \inputminted[frame=single]{coq}{Listings/Airlock-out.nusmv}

  An important feature of model checkers is that they generate a counter-example
  when they find a sequence of transitions which does not satisfy the
  specification.
  %
  For instance, if we slightly modify the NuSMV model of our airlock system so
  that it is no longer require for the second door to be close in order to open
  the first door, then the model checker generates the following
  counter-example:

  \inputminted[frame=single]{coq}{Listings/Airlock-counter.nusmv}
\end{example}

\paragraph{Theorem Proving}
%
A theorem prover is a computer program to write machine-checked proofs in a
particular formal logic, that is a formal language, to express statement, a set
of axioms, and a collection of inference rules to derive new formulas from the
axioms.
%
A theorem prover' underlying logic is more expressive than what model checkers
offer, but they lack automation for nontrivial problems.
%
They do not excuse users from \emph{writing proofs}, which can be very important
in terms of effort for real world systems.

\begin{example}[Airlock System Verification in Coq]
  The Coq theorem prover includes two languages: a rich functional language to
  write specifications ({\sc Gallina}) and a tactic language to write proofs
  ({\sc Ltac}).
  %
  We can {\sc Gallina} to describe the system in terms of states and actions to
  update this state.

  \inputminted[frame=single]{coq}{Listings/Airlock1.v}

  This reads as follows: a door is either open or close; a airlock system is
  made of two doors; we consider four actions which imply an update of the
  airlock system's state (opening the first or second door, closing the first or
  second door).

  {\sc Gallina} includes a special type, called \texttt{Prop}, to write logical
  propositions which can be either true or false.
  %
  We can define the transition relation of our airlock system using
  \texttt{Prop.}
  %
  In practice, this is done by enumerating the cases where the statement is
  true, and the remaining cases are considered false.

  \inputminted[frame=single]{coq}{Listings/Airlock2.v}

  This reads as follows: we distinguish between four classes of transitions, one
  per actions.
  %
  On the one hand, a transition which implies to open a door (case one and
  three) requires the both doors to be closed.
  %
  On the other hand, a transition which implies to close a door (case two and
  four) requires the door to be closed.

  The security property of our airlock system is a safety property: at any time,
  at least one door is close.
  %
  This property is a predicate on the system state, which we can define in {\sc
    Gallina} \emph{via} the \texttt{Prop} type.

  \inputminted[frame=single]{coq}{Listings/Airlock3.v}

  For the airlock to be secure, this means the \texttt{secure\_airlock}
  predicate must hold true before and after each transition.
  %
  We can express that in the form of a \emph{lemma}, that is a logical statement
  for each we exhibit a proof.
  %
  We write the proof with {\sc Ltac}, and trust Coq to refuse it in case it is
  incorrect.
  %
  The proof is written in terms of {\sc Ltac} tactics.
  %
  Each tactic corresponds to a reasoning rule.
  %
  For instance, the \texttt{right} tactic refers to the fact that given two
  statements $P$ and $Q$, then if $Q$ is true, then $P \vee Q$ is also
  true. Tactics updates the proof obligation (initially the lemma statement),
  and the goal of a Coq user is to bring back this proof obligation to something
  that has already been proven.

  \inputminted[frame=single]{coq}{Listings/Airlock4.v}

  This reads as follows: we prove that, given any ``secure state'', that is a
  airlock for which the \texttt{secure\_airlock} holds true, any action which
  implies a system's state update, the \texttt{secure\_airlock} also holds true
  for the system's state after the transition.
  %
  The proof is conducted by enumeration of possible actions
  (\texttt{induction}), and by definition of \texttt{Transition}
  (\texttt{inversion}).
\end{example}

\subsection{Examples of Large Systems' Verification}

\paragraph{\ac{xom}.}
%
The \ac{xom} microprocessor architecture maintains separate so-called
\emph{compartments} for applications.
%
With mainstream microprocessor architectures, the system software is responsible
for both memory allocation and access control.
%
It relies on configurable \ac{cpu} mechanisms, such as a \ac{mmu}, to implement
the latter.
%
On the contrary, a \ac{xom} \ac{cpu} keeps track of each memory location owner,
thanks to a tagging mechanism, and prevents an application to access a memory
location it does not own.

In 2003, David Lie \emph{et al.} have modelled the \ac{xom} architecture using
the Mur$\varphi$ model checker.
%
This model follows the principle of a \ac{lts}: a set of states, a set of
labelled transitions (called \emph{rules} in the context of this work), and a
set of transitions.
%
There are two kinds of rules: the \ac{xom} instructions set, and some additional
capabilities given to the attacker.
%
As for the transitions set, it is defined in terms of pre and postconditions.
%
On the one hand, the precondition is parameterized by the current state and the
labelled event.
%
If the pre-condition is satisfied for a given hardware state and labelled
events, then it means that event can occur in this state.
%
On the other hand, the post condition is parameterized by the initial state, the
labelled event and the resulting state.
%
It determines the consequences of that event, in terms of state update.

The security properties targeted by the \ac{xom} architecture are enforceable
security properties, and the authors rely on Mur$\varphi$ to perform the state
exploration of their model.
%
The main advantage of a model checker, in this context, is to be able to print
the trace it has found not to satisfy the targeted security property.
%
This trace describes an attack path, that is the execution steps to reproduce in
order to defeat the security enforcement.
%
Hence, the authors have been able to show with their model that the \ac{xom}
architecture was subject to several replay attacks, and they leveraged their
model to validate their countermeasures.
%
However, the state explosion problem obliges the authors to simplify their
model, in order to reduce the state combinatory.

\paragraph{VirtCert}
%
Between 2011 and 2014, Gilles Barthe \emph{et al.} have worked on an idealized
model of a hypervisor.
%
This model is defined in terms of state, actions and a semantics of actions as
state-transformers.
%
That is, even if the authors are not using the terms \ac{lts}, their model
follows the same principles.
%
In their context, the state mixes information about both hardware components
(\ac{cpu} execution mode, registers, memory content, etc.) and software
components (list of guests, current active guest, memory mapping for the
hypervisor and the guests, etc.).
%
The actions are the hypercalls, exposed by the hypervisor for the guests to use.
%
The semantics of actions as state-transformers, that is the set of possible
transitions for the modelled system, is defined in terms of pre and
postconditions.

First, they have shown that their ideal hypervisor was
%
\begin{inparaenum}[(1)]
\item ensuring strong isolation between guests, and
%
\item eventually processing every request performed by the
  guests\,\cite{barthe2011virtcert1}.
\end{inparaenum}
%
Then, they have incorporated the \ac{cpu} cache to their
model\,\cite{barthe2012virtcert2}.
%
Cache-based attacks, where attackers are able to infer information they should
not have access to by leveraging their knowledge of micro-architectural
implementation specificity, are a very important threat to virtualization
platforms.
%
The authors have shown their ideal hypervisor could prevent such attack, at the
cost of flushing the cache before each context switch.
%
They have taken their approach a step further, by focusing on constant-time
cryptography\,\cite{barthe2014virtcert3}.

The scope of this hypervisor model is large, and cover many security aspects
primordial for virtualization platforms.
%
Moreover, there have been an important specification and formalization effort
required to take into consideration the different security properties.
%
Some of them, like constant-time cryptography implementations, are not
enforceable security properties.
%
Indeed, it is not possible, only with the trace of one execution, to know
whether a given implemantion is constant time.
%
It is required to consider all the possible traces.

\subsection{Challenges of HSE Mechanisms Verification}

The main characteristic of \ac{hse} mechanisms is that they are the result of
both hardware and software components, in presence of a larger system with
untrusted actors.
%
Hardware and software components are not of the same nature.
%
Hardware components are physical devices which accept inputs and compute
outputs.
%
Software components are pieces of data scattered into memory spaces, whose
semantics are determined by the processor unit that executes them.
%
While both hardware and software components have been subject to formal
verification in the past, these verification works tend to rely on different
representations.
%
Florian Lugou \emph{et al.} explain this in depth while they introduce SMASHUP,
a toolchain for unified verification of hardware-software
co-designs\,\cite{lugou2017smashup}.
%
This is why we have decided to focus on the specification level in the context
of this thesis.
%
Refining our potential conclusions up to a concrete implementation is necessary
in the long term.
%
Fortunately, it is also a very active field, with impressive flagship projects
such as DeepSpec\,\cite{appel2017deepspec}.

\section{Design Methods Easing the Verification}
\label{sec:related:ease}

\subsection{Monadic Modelling}

\begin{itemize}
\item State Monad
\item Hoare Monad
\end{itemize}

\subsection{Component-based Modelling}

\section{Conclusion}

\newpage

\section{Labelled Transition System} % ========================================
\label{sec:related:lts}


\paragraph{}
%
Both \ac{xom} model and VirtCert rely on \acp{lts} or similar formalism to
specify one component in particular and to verify a set of targeted properties.
%
However, in their models, all the trusted components are specified and verified
together.
%
From a \ac{xom} architecture perspective, there is no such thing as a
``trusted'' software component, because the access control mechanism is solely
implemented by the \ac{xom} \ac{cpu}.
%
As for VirtCert, the model purpose is to validate a determined hypervisor model.
%
From this thesis perspective, a hypervisor is a trusted software component which
implements a \ac{hse} mechanism by correctly configuring hardware mechanisms
such as virtualization technologies.
%
If we were eventually able to formally specify a HSE mechanism which was proven
to correctly implement guests isolation, then we could verify that VirtCert
hypervisor satisfies the software requirements of the HSE mechanism.
%
The main idea is to be able to reuse the same software requirements for a
totally different system software, which also happens to use the same
virtualization technologies to enforce guests isolation, but a different
partition algorithm.

\ac{lts} have also been used to specify requirements over software components,
and verify these requirements were sufficient for the hardware architecture to
enforce a set of targeted properties.

\paragraph{RockSalt}
%
Thanks to the Google's service called \ac{nacl}, it is possible for software
developers to distribute their web applications in the form of native executable
code.
%
These native applications are executed directly in the context of the browser.
%
\ac{nacl} uses \ac{sfi} to prevent arbitrary native applications to tamper with
the code and data of the browser.
%
\ac{sfi} comprises a set of rules native applications have to comply with, and
that together form a sandbox policy.
%
In practice, the \ac{nacl} checks that an arbitrary native applications respect
the \ac{sfi} rules before loading it to the browser context.
%
As a consequence, the browser is protected from malicious machine code.

From a security perspective, this means there are two requirements over the
\ac{nacl}:
%
\begin{inparaenum}[(1)]
\item verify that the rules indeed are sufficient to constrain the untrusted
  native applications with respect to the sandbox policy, and
%
\item the \ac{nacl} checker correctly verify that native applications respect
  these rules.
\end{inparaenum}
%
This is similar to the challenges faced by \ac{hse} mechanisms.
%
Because \ac{nacl} has been shown to have issues regarding these two
requirements, Greg Morrisett \emph{et al.} have specified the \ac{nacl} checker
in Coq, proven the rules it checks are correct and indeed enforce the targeted
sandbox policy, and then manually translated the checker in
C\,\cite{morrisett2012rocksalt}.
%
By doing so, they have addressed the two requirements listed above, as long as
their translation is correct.
%%
% They proceeded as follows.
%%
% First, they modelled the set of states of the CPU.
%%
% Then, they defined a translator, from the complex x86 instruction set to a
% much simpler instructions set, easier to reason with.
%%
% They define a semantics for this simpler instructions set.
%%

\paragraph{Moat}
%
Intel \ac{sgx} is a recent addition to certain x86 \acp{cpu}, whose purpose is
to provide so-called enclaves to user land
applications\,\cite{costan2016sgxexplained}.
%
These enclaves supposedly offer an execution environment isolated from the
system software.
%
The functioning of \ac{sgx} can be roughly summarized as follows:
\ac{sgx}-capable CPU dedicates a special portion of the system memory, called
the \ac{epc}, to enclaves.
%
The system software is responsible for allocating and initializing memory pages
of the \ac{epc} (thanks to dedicated instructions), but it cannot read or write
to them once it is done.
%
This is enforced by the memory controller, which encrypts the content of the
\ac{epc} transparently from the \ac{cpu}.
%
Hence, the memory controller only decrypts an \ac{epc}'s page if it is accessed
by the enclave which owns it; it will also discard write accesses performed by
another software component than the page owner.

Rohit Sinha \emph{et al.} have modelled SGX instructions semantics, to complete
an existing model, and have developed an automated information flow analysis
tool called Moat, to verify whether are not a given application code may leak
secrets or not\,\cite{sinha2015moat}.
%
The work proceeds similarly to RockSalt, where instructions are treated as
labelled transition from one state to another.
%
However, Moat also consider an active and passive adversary, with additional
capabilities (meaning, additional transitions in the system).
%
The approach is similar to what David Lie \emph{et al.} have done for the
\ac{xom} architecture.

\paragraph{}
%
Both RockSalt or Moat express requirements software requirements shall comply
with in order to enable hardware to enforce a targeted security policy.
%
Moat, in particular, allows for considering several software components, in the
form of an active adversary with system software capabilities.
%
We are willing to generalize their approach, to specify and verify \ac{hse}
mechanisms.
%
However, because we target a larger scope, we will remain at the specification
levels, and we do not plan on verifying particular implementations of trusted
software components.

Each work we have been introducing in this Section relies on a model of a
hardware architecture.
%
This model often comprises the hardware features that are directly required by
the software component.
%
Because we mostly focus on architectural attacks, and because architectural
attacks leverage unsuspected compositions of hardware features, we cannot adopt
this approach for the long term.
%
Defining a model which is comprehensive in terms of hardware components, and
usable for verifying relevant properties, calls for methodological requirements.
%
Indeed, the more complex a model becomes, the more challenging it is to reason
with.

\section{Hoare Logic} % =======================================================
\label{sec:related:hoare}

Model complexity originates in two situations:
%
\begin{inparaenum}[(1)]
\item when transitions from one state to another imply important state updates,
  and
%
\item when the state comprises an important number of sub-components.
%
\end{inparaenum}
%
This is reminiscent of the programming language problematic to model large and
verify large programs with side effects.
%
In this context, the execution of functions modifies the state of the program
context, made of its stack, heap, etc.
%
Floyd-Hoare Logic (more commonly, Hoare Logic) is a popular formal system for
reasoning about the correctness of computer programs.
%
The system is defined in terms of state and commands, and a semantics of
commands as state-transformers defined in terms of pre and postconditions.
%
A command can be an axiomatic operation (\emph{e.g.} an assignment statement in
a computer program), or a composition rule (\emph{e.g.} a \emph{if-else}
statement, a function call, etc.).
%
Hoare Logic allows for modular reasoning.
%
Once a couple of pre and postconditions is proven for a given command, further
reasoning wherein this command appears can be based solely on these pre and
postconditions.
%
As a result, the concrete implementation of the command is abstracted away.
%
Defining our hardware model in an imperative style has also the extra benefit of
making it closer to what domain experts are familiar to, compared to \emph{e.g.}
functional programming.

\paragraph{Frama-C}
%
The Framework for Modular Analysis for C programs (Frama-C) is a set of C
programs analysers.
%
Several analysers (\emph{e.g.} WP, Value) are based on Hoare Logic.
%
The program source code is annotated with pre and postconditions, defined in a
dedicated language called \ac{acsl}.
%
The goal of a Frama-C analyser is to conclude whether the postcondition of a C
statement is enforced by the precondition.
%
The proof can be computed automatically for simpler problems, or interactively
\emph{via} external tools.
%
For instance, Frama-C's analysers can be configured so that they formulate the
predicates they are not able to prove themselves in Coq lemmas for the user to
prove.
%
If the user is able to write a proof accepted by Coq, then Frama-C can use this
result.

\paragraph{}
%
Using the C language to reason with hardware components is not without
precedent.
%
For instance, the reference implementation of Scalable Coherent Interface (SCI)
Cache Coherence Protocol\,\cite{stern1995cachecoherence} is written in C.
%
However, C is a (relatively) low-level language.
%
This makes reasoning with higher-level abstractions more difficult.
%
For instance, a list is a common and useful data structure.
%
In C, lists are usually encoded as linked list, which implies reasoning with
pointers (and potential memory aliasing, etc.).
%
Using a higher-level language can be useful to avoid this problem.
%
Functional programming languages easily qualify as ``higher level'' than C.
%
In this context, reasoning about state and side effects is usually achieved
thanks to monads\,\cite{jones2005io}.
%
Monads allow for modelling stateful computations, either \begin{inparaenum}[(1)]
%
\item from axiomatic operations which locally manipulate the state, or
%
\item thanks to a composition operator called \emph{bind}, which creates a new
  computation by chaining two simpler ones.
%
\end{inparaenum}

\paragraph{Ynot}
%
Ynot is a Coq framework which applies Hoare Logic to monads, so it becomes
possible to formally reason with side effects in Coq\,\cite{chlipala2009ynot}.
%
Programs with side effects are defined in terms of the $\mathtt{Cmd}$ monad,
where the type of a computation not only tells the returned type, but also a
couple of pre and postconditions that specifies the effects of the computation
on the program's heap.
%
When a Ynot user uses \emph{bind} to chain two existing computations, it has to
prove the post condition of the first computation satisfies the precondition of
the second one.
%
As a consequence, programs defined in terms of the $\mathtt{Cmd}$ monad are
necessarily correct by construction.
%
The authors claim that writing a program with Ynot and the $\mathtt{Cmd}$ monad
is often not much harder than writing that program in Haskell and the
$\mathtt{IO}$ monad.
%
This is achieved thanks to specific automation tactics written to ease the
deductive reasoning required by \emph{bind}.

\paragraph{Pip}
%
Pip is a minimal kernel whose reference implementation is written in {\sc
  Gallina}, the specification language of Coq.
%
The authors have verified their reference implementation correctly configures a
\ac{mmu} in order to isolate the applications it manages\,\cite{jomaa2016mmu}.
%
The objective is similar to the first iteration of
VirtCert\,\cite{barthe2011virtcert1}, However their approach to specify the
system software is closer to what Ynot has proposed.
%
The Pip system calls are defined in terms of a dedicated monad, which allows for
manipulating a mix of hardware state (notably including the \ac{mmu} current
configuration) and Pip internal state (\emph{e.g.} shadow \ac{mmu} tables with
additional information).
%
The correct isolation of applications is defined as a predicate on this state,
and they prove this predicate is an invariant during Pip execution:
%
from a ``secure'' state, the new state resulting of the execution of any Pip
system call is ``secure.’’

\paragraph{}
%
It is possible to ease the reasoning about a complex system, by specifying the
system transitions as compositions of simpler, reusable state updates.
%
Thanks to Hoare Logic, it is possible to abstract away these state update with a
couple of pre and postconditions.
%
In addition, compared to other approaches described in
Section~\ref{sec:related:lts}, the resulting specification is written in an
imperative style, more familiar for system software developers or hardware
designers.
%
However, the result works presented above all consider the whole system state at
the same time.
%
For instance, both VirtCert and Pip consider a mix of hardware and software
states.
%
In our case, one of our objectives is to consider the hardware architecture as a
whole, and to do that we need to use a model which is comprehensive in terms of
hardware components.
%
Hardware architectures are often designed as a hierarchy of hardware components.
%
Defining a couple of pre and post conditions for a computation which happens
locally inside one component, but may affect other sub-components in the
process, becomes challenging.
%
To overcome this challenge, we need to be able to abstract totally these
components.

\section{Component-based Modelling} % ==========================================
\label{sec:related:interface}

Previous approaches flatten the state of the system.
%
This design choice has two important consequences:
%
\begin{inparaenum}[(1)]
%
\item it reduces modularity by explicitly coupling all the system components
  together, and
%
\item it encourages reducing the scope of the model to only take into account
  the hardware features which are directly required.
%
\end{inparaenum}
%
This goes against the Separation of Concerns principle, where the state of a
given component is less important than its behaviour to requests.
%
That is, by focusing on components' interaction, we can specify and verify each
component individually.
%
This approach forces us to specify requirements over components the target is
directly connected to.
%
We then can reason about components’ composition, by verifying that the
components indeed hold the requirements formulated previously.
%
Eventually, we will be able to model the complete computing platform.
%
One very important advantage of this approach over ``flatten state'' is that it
structure the reasoning and highlights the share of responsibility of each
component in the context of enforcing a targeted security property.

\paragraph{Component-based Modelling}
%
We have found a very good illustration of this approach in the work of Thomas
Heyman \emph{et al.}\,\cite{heyman2012securemodel}.
%
They have presented a component-based modelling technique for
Alloy\,\cite{jackson2012alloy}, where components are primarily defined as
semantics for sets of operations.
%
One component is connected to another when it leverages its operations.
%
The goal of the analysis is to reason about component composition, in particular
with respects with security requirements.
%
These requirements are defined in terms of predicates on operation results.
%
One very interesting feature of Alloy is that it allows for reasoning about
partial models.
%
This is done by specifying a set of requirements over the operations results, in
place of a complete implementation.
%
From our perspective, this is useful for at least two situations:
%
\begin{inparaenum}[(1)]
\item it enables incremental verification, where the system is specified
  component by component
%
\item it can be leveraged to ``fill the gap'' of a system where certain
  components are under-specified, or their implementations are proprietary.
%
\end{inparaenum}
%
Because of the scale of a hardware architecture, incremental modelling and
verification is a game changer.
%
At the same time, being able to model completely and accurately each hardware
component is unlikely, as proprietary components are the norm, while open
hardware remains a niche market.
%
At least, it becomes possible to clearly define the assumptions that are made
about these black boxes

Unfortunately, Alloy does not have a mechanism similar to, \emph{e.g.} Coq code
extraction, that would enable model validation.
%
Being able to validate the model of a component, whether it is a hardware or a
software component, is a desirable feature when the latter exists prior to the
former.

\paragraph{Kami}
%
As part of the DeepSpec project\,\cite{appel2017deepspec}, Joonwon Choi \emph{et
  al.} have released Kami\,\cite{choi2017kami}.
%
Kami's main objective is to bring software formal verification technique based
on proof assistants to hardware conception; a world that is still dominated by
model checking approaches.
%
The result is a framework for the Coq proof assistant, to implement and verify
hardware components.
%
Pushing the \texttt{Notation} feature of Coq, which let the developer extend the
proof assistant parser with its own construction, to its maximum, they offer a
development environment very close to BlueSpec\,\cite{nikhil2004bluespec}.
%
Kami's hardware component is specified as labelled transition systems, whose set
of transition labels forms its interface.
%
A transition is defined in terms of actions, which can be local to a component,
or consists of interacting with another component.
%
Finally, Kami enables modularity, by allowing for substituting one module $m$ by
another module $m'$, if the latter is proven to be a refinement of the former.
%
For a given component of the computing platform, it becomes possible to reason
about the rest of the system in terms of high-level, specification modules.
%
These modules are as many hypotheses which can be confirmed by proving the
related subsystems effectively refine them.

BlueSpec is a target for Kami's models extraction.
%
Therefore, it s possible to generate FPGA bitstream from a Kami module, using
the BlueSpec compiler.
%
Although Kami allows for more components’ connection patterns than what we
eventually propose in the context of this thesis, it is hardware-specific, thus
is not suitable for reasoning about systems which also contain software
components.
%
In addition, its purpose is to verify hardware component implementation, while
we rather stay at the specification level.

\paragraph{}
%
In a similar manner than for interconnected hardware components, side effects of
a computer program often rely on an ``outer'', \emph{stateful} environment.
%
For instance, when an application reads the content of a file, it does not
perform most of the work, the system software does.
%
Taking this environment into account is challenging.
%
For instance, Frama-C does not provide a mature analyser to that end.
%
In functional programming languages, algebraic effects are a recent and popular
approach whose purpose is to tackle these
challenges\,\cite{brady2014effects,bauer2015effects}.
%
They allow modelling large classes of effects, and composing these effects
within purely functional languages, while deferring the realizations of these
effects to dedicated handlers.

They are several languages with an implementation of algebraic effects,
including Eff, Idris, Haskell, PureScript.
%
However, none of them provides a proof environment comparable to what Coq
proposes.
%
To our surprise, we did not find any comprehensive approach to write and verify
programs with effects and effect handlers written for {\sc Gallina}.

\paragraph{Coq.io}
%
In 2015, Guillaume Claret \emph{et al.} have released Coq.io, a framework for
the Coq proof assistant to write programs with side effects and
concurrency\,\cite{claret2015coqiowww}.
%
Similarly to Ynot, the authors model side effects with axiomatic monadic
operations.
%
However, they do not attempt to model the impacts of these operations over the
program state, and only the returned type of each operation is specified.
%
Their objective is to consider operations which rely on an ``outer'' environment
to complete.
%
For instance, reading the content of the standard input is such an operation, as
it is implemented with a system call, and therefore implies the operating
system.
%
To reason about the correctness of a program which uses such operations, they
generalize the concept of use cases of unit testing, in the forms of so-called
scenarios\,\cite{claret2015coqio}.
%
A scenario is the formalization of requirements over the environment responses,
for a given sequence of operations.
%
It becomes possible to verify that the program ``is correct'' under the
hypotheses modelled with these scenarios.
%
Coq.io has been developed with the code extraction feature of Coq in mind.
%
The framework introduces several axiomatic operations for Coq, and a OCaml
library to provide concrete implementations for these operations.
%
However, in its current state, Coq.io provides no tool for verifying if the
scenarios are indeed realistic.
%
That is, we cannot verify the composition of a program specified in Coq using
Coq.io, and then extracted as a OCaml module, with \emph{e.g.} a given operating
system.

\section{Conclusion} % ========================================================

The common approach to formally specify a system is by defining transition
system which describes its behaviour.
%
By reasoning about its potential executions, that is the set of traces of its
transition system, it becomes possible to verify targeted security properties.
%
Some security properties may be defined in terms of predicate on the system
executions.
%
Others require to be defined in terms of predicate on the set of the system
executions.

Our initial objective is to propose a generic approach to specifying and verify
HSE mechanisms, which are the result of a hardware-software cooperation.
%
In this context, and because of the very nature of the architectural attacks we
are willing to prevent, we have to consider the computing platform as a whole.
%
The scale of the tasks obliges us to take extra care when defining the
transition system.
%
Modelling the software and hardware components of a computing platform as
programs with effects and effect handlers appears as a promising solution to
solve this problem.
%
However, modularly specifying and verifying programs with effects and effect
handlers in Coq remains an open challenge.
