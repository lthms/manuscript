%!TEX root = ../main.tex
\chapter{Specifying and Verifying a BIOS HSE Mechanism}
\label{chapter:speccert2}

\endquote{``\emph{BIOS as the root of trust. For everything.}''

  \hfill\footnotesize --- Joanna Rutkowska}

\vspace{1cm}%
\noindent
%
In Chapter~\ref{chapter:speccert}, we have introduced a theory of \ac{hse}
mechanisms, whose purpose is to be the foundation of a three-step methodology to
formally specify and verify \ac{hse} mechanisms.
%
We have also introduced a code injection policy for a software stack made of a
\ac{bios}, an operating system and \( n \) applications
(Definition~\ref{definition:speccert:isolation}), and we have defined a
sub-policy which focuses on the \ac{bios} protection
(\ref{def:speccert:biospol}).
%
In this Chapter, we apply our approach to specify and verify the \ac{hse}
mechanism implemented by the \ac{bios} on x86 hardware architectures at runtime
and described in Section~\ref{sec:usecase:hse}.
%
The purpose of this \ac{hse} mechanism is to provide an isolated execution
environment to the \ac{bios}. It should therefore enforce the so-called
\ac{bios} code injection sub-policy.

The rest of this Chapter proceeds as follows.
%
First, we define a minimal x86 hardware model we call {\scshape Minx86}, whose
scope comprises the hardware features required to express our case study
(Section~\ref{sec:speccert2:minx86}).
%
Then, we give a formal definition of the \ac{hse} mechanism implemented by the
\ac{bios} at runtime, and show that this definition satisfies the two \ac{hse}
laws and is correct with respect to the targeted security policy previously
introduced (Section~\ref{sec:speccert2:verif}).
%
The model and the proofs described in this Chapter have been implemented using
Coq.
%
We have released this development as free software in a project called
SpecCert\,\footnote{SpecCert can be found at the following URL:
  \url{https://github.com/lthms/speccert}}.
%%
% As we introduce new definitions and theoretical results, we point to the
% related statements in SpecCert, so the interesting reader familiar with Coq
% can refer to our machine-checked proofs.

\section{A Minimal x86 Hardware Model}
\label{sec:speccert2:minx86}

$\formatLTS{Minx86}$ is intended to be a minimal model for single core x86-based
machines and we have used publicly available Intel
documents\,\cite{intel2013celeron,intel2009mch,intel2014manual} to define it.
%
The hardware architecture we are modeling with $\formatLTS{Minx86}$ matches the
one depicted in Figure~\ref{fig:speccert:smram} and contains a core, a cache, a
memory controller, a \ac{dram} controller and a VGA controller\,\footnote{A VGA
  controller is a hardware device which on we can connect a screen. It exposes
  some memory to the core for communication purposes.} which both expose some
memory to the core.
%
In its current state of implementation, its scope focuses on the \ac{smm}, but
{\scshape Minx86} is intended to be incrementally extendable to cover more
hardware features.

The rest of this Section proceeds as follows.
%
We give an overview of the {\scshape Minx86} scope (\ref{sec:speccert2:scope}),
then describe in depth its key components of the LTS, that is its set of states
(\ref{subsec:speccert2:state}) on the one hand, and its set of labeled
transitions (\ref{subsec:speccert2:labtrans}) on the other hand.
%
Finally, in order to reason about code injection policies
(Definition~\ref{def:speccert:global}), we define the transition-software
mapping of {\scshape Minx86} (\ref{subsec:minx86:fetched}).

\subsection{Model Scope}
\label{sec:speccert2:scope}

\paragraph{Hardware Features.}
%
We consider the core can be either in System Management Mode (\ac{smm}) or in an
unprivileged mode.
%
As explained in Section~\ref{subsec:usecase:firm:sec}, when a processor receives
a System Management Interrupt (SMI), it halts its current execution and
reconfigures itself to a hard-coded state.
%
Then it executes the code stored in memory at the address
$\mathtt{SMBASE} + \val{0x8000}$.
%
In practice, the SMBASE value points to the beginning of a memory region called
the SMRAM.
%
Leaving the \ac{smm} is done by executing a special purpose instruction called
\texttt{rsm} (for \emph{resume}).

The core relies on a cache to reduce the Input/Output (\IO, that is a read or
write access to the memory) latency.
%
We model one level of cache which stores both data and instructions and we
consider two cache strategies: uncacheable (UC) and writeback (WB).
%
With the UC strategy, the cache is not used and all \IOs are forwarded to
the memory controller, whereas with the WB strategy, the cache is used as much
as possible\,\footnote{These cache strategies are explained in
  \cite{intel2014manual}, Volume 3A, Chapter 11, Section 11.3 (page 2316 --
  2317)}.
%
To determine which cache strategy to use, the core relies on several
configuration registers.
%
Among them, the \ac{smrr} tell the core where the SMRAM is and which
cache strategy to use for \IO targeting the SMRAM when the core is in \ac{smm}.
%
When it is not in \ac{smm}, the core always uses the UC strategy for \IO
targeting the SMRAM.
%
Such registers can only be configured when the core is in \ac{smm}.
%
\ac{smrr} have been introduced as a countermeasure to the SMRAM cache poisoning
attack\,\cite{wojtczuk2009smram,duflot2009smram} which allowed some untrusted
code to tamper with the copy of the SMRAM stored in the cache.
%

The memory controller\,\cite{intel2009mch} receives all the core \IOs which are
not handled by the cache and dispatches them to the DRAM controller or to the
VGA controller.
%
It exposes a unified view (the memory map) of the system memory to the core
%
The core manipulates this memory map with a set of physical addresses.
%
The memory controller uses a special range of physical addresses to host the
SMRAM.
%
This memory area is dedicated to store the code intended to be executed when the
core is in \ac{smm}.

\paragraph{Tracking Memory Ownership.}
%
The \formatLTS{Minx86} definition is parameterized by a hardware-software
mapping \func{context} (Definition~\ref{def:speccert:hardsoft}).
%
The memory locations of \formatLTS{Minx86} are either cache lines or memory
cells exposed by the \ac{dram} or the VGA controllers.
%
The memory ownership is updated through transitions according to three rules:
%
\begin{enumerate}
\item When a cache line gets a copy of a \ac{dram} or VGA cell content, the
  owner of this cell becomes the new owner of this cache line.
%
\item When the content of this cache line is written back to a memory cell, the
  new owner of this memory cell is the owner of this cache line.
%
\item During a software transition whose purpose is to overwrite the content of
  a memory location, the software component currently executed ---and therefore
  responsible for the software transition--- becomes the new owner of this
  memory location.
%
\end{enumerate}

\begin{definition}[\formatLTS{Minx86}]
  \label{def:speccert2:minx86}
  We write $\formatLTS{Minx86}(\func{context})$ for the hardware model which
  tracks memory ownership with respect to the \func{context} hardware-software
  mapping, such that
%
  \[
    \formatLTS{Minx86}(\func{context}) \triangleq \langle H(S), L_S, L_H,
    \xrightarrow[\func{context}]{} \rangle
  \]
  %
  where $S$ is the image of $\func{context}$, that is the set of software
  components which form the software stack.
\end{definition}

We emphasize that there are many valid definitions of the \func{context} function,
regarding the verification problem we consider.
%
For instance, we need two different definitions of \func{context} whether we
consider a hypervisor in the software stack or not.
%
We do not rely on the concrete definition of \func{context} to implement the
tracking of memory ownership within {\scshape Minx86}, hence it is left as a
parameter for the model.

\subsection{Hardware States}
\label{subsec:speccert2:state}

$H(S)$ is defined as the Cartesian product of the set of states of the core, the
cache, the memory controller and the memories exposed by both the \ac{dram}
and the VGA controllers.
%
Each of these sets is defined in order to model the hardware features we have
previously described.
%
We use named tuples\,\footnote{See page \pageref{frontmatter:notations} for a
  description of the notations we used to manipulate them.} to
define them.
%
Thus, $H(S)$ is defined as follows:
%
\[
  H(S) \triangleq \left<\ \func{core}: \mathtt{Core},\quad\func{cache}:
    \mathtt{Cache}(S),\quad\func{mc}: \mathtt{MC},\quad\func{mem}:
    \mathtt{Mem}(S) \ \right>
\]
%
\( \mathtt{Cache} \) and \( \mathtt{Mem} \) are parameterized by \( S \) because
they expose memory locations to the core which can contain instructions to be
fetched, and to implement the tracking of memory ownership within our model, we
need to taint the memory locations with their respective owner.
%
We do not consider core registers in {\scshape Minx86} yet.
%
When we extend our model to that end, we will need to taint them accordingly,
and \( \mathtt{Core} \) will have to be parameterized by \( S \) too.

\paragraph{Address Spaces.}
%
We define $\texttt{PhysAddr}$ the set of physical addresses the core uses to
perform \IO and $\texttt{HardAddr}$ the set of hardware addresses exposed by the
\ac{dram} and VGA controllers.
%
To distinguish between \ac{dram} and VGA addresses, we use two different
constructors\,\footnote{See page \pageref{frontmatter:notations}.}.
%
\[
  \begin{array}{rcl}
    \texttt{PhysAddr}
    & \triangleq
    & \textsc{Pa} : [0, \val{max\_addr}] \rightarrow
      \texttt{PhysAddr} \\
      % --------------------------------------------------------------------------
      % no type name
    & % no symbol
    & \\ % no constructor
    % --------------------------------------------------------------------------
    \texttt{HardAddr}
    & \triangleq
    & \textsc{Dram} : [0, \val{max\_addr}] \rightarrow
      \texttt{HardAddr} \\
      % --------------------------------------------------------------------------
      % type name
    & |
    & \textsc{Vga} : [0, \val{max\_addr}] \rightarrow
      \texttt{HardAddr}
  \end{array}
\]
%
The maximal address offset (denoted by $\val{max\_addr}$ here) is specific to
the core and may vary in time according to its addressing mode (real mode, long
mode, etc.), therefore we leave its value as a parameter of our model.
%
By convenience, we give the same maximum address to each address space.

Finally, we write $\texttt{Val}$ for the set of values that the memory cells
scattered within the memory locations of the hardware architecture can take.

\paragraph{Core.}
%
The set of states of the core is denoted by $\texttt{Core}$.
%
We give a minimal definition of $\texttt{Core}$, with a clear focus on the
\ac{smm}.
%
\[
  \texttt{Core} \triangleq \left<\
    \begin{array}{c}
      \func{in\_smm} : \setshortdef{\val{true},
      \val{false}},\quad\func{pc}: \texttt{PhysAddr},\quad\func{smbase} :
      \texttt{PhysAddr}, \\
      \func{smrr} : \texttt{Smrr},\quad\func{strat} :
      \texttt{PhysAddr} \rightarrow \texttt{CacheStrat}
    \end{array}
  \right>
\]
%
The boolean $\func{in\_smm}$ is set to $\val{true}$ when the core is in \ac{smm}
and to $\val{false}$ otherwise.
%
The physical address $\func{pc}$ models the program counter, a register used to
store the address of the next instruction to be fetched and executed.
%
The physical address $\func{smbase}$ models the register of the same name.
%
The map $\func{strat}$ abstracts away the numerous mechanisms of x86
microprocessors to determine which cache strategy to use for a given \IO, where
$\texttt{CacheStrat} \triangleq \setshortdef{\val{UC}, \val{WB}}$ is the set of
the modelled cache strategies.
%
The set of states of the \acp{smrr} is denoted $\texttt{Smrr}$.
% \TODO{SMRR est noté ici en majuscule mais la variable censé les modélisé est
% en minuscule.}
%
% Thomas : Ça ne me pose personnellement pas de problème, l'un est un acronyme
% et l’autre une variable justement.
%
\[
  \texttt{Smrr} \triangleq \langle~\func{range}:
  \powerset(\texttt{PhysAddr}),\quad\func{strat}: \texttt{CacheStrat}~\rangle
\]
%
The set of physical addresses $\func{range}$ tells the core the location of the
SMRAM and $\func{strat}$ indicates which cache strategy has to be used when the core
is in \ac{smm}.

% which can be available or already used. A cache line already used is tagged
% with the physical address of its content. For a given physical address, the
% \ac{cpu} computes an index to select a cache line and verify its tag.  hich
% contain, in addition to the copy of the cached memory content, a "dirty bit"
% and a tag. A cache line is marked as dirty when its content is modified which
% indicates if the cache line content has been modified without propagating the
% underlying hardware memories and the .

\paragraph{Cache.}
%
Let $\texttt{Index}$ be the set of cache indexes and
$\func{index} : \texttt{PhysAddr} \rightarrow \texttt{Index}$ the function used
by the core to determine which index to use for a given physical address.
%
They are both parameters of our model.
%
The cache is divided into several cache lines which contain the cached memory
content and several additional information required by the cache strategy
algorithm.
%
The set of states of the cache line is denoted by $\texttt{CacheLine}(S)$.
%
In addition to modeling the hardware specifications, the definition of
$\texttt{CacheLine}(S)$ attaches a software owner to a cache line.
%
\[
  \texttt{CacheLine}(S) \triangleq \langle~\func{dirty} :
  \setshortdef{\val{true}, \val{false}},\quad\func{tag} :
  \texttt{PhysAddr},\quad\func{content}: \texttt{Val},\quad\func{owner}:
  S~\rangle
\]
%
The cache is modelled as a mapping between address indexes and cache lines.
%
\[
  \texttt{Cache}(S) \triangleq \texttt{Index} \rightarrow \texttt{CacheLine}(S)
\]
%
A cache \( c \in \texttt{Cache}(S) \) is well-formed if every cache line is
tagged with a physical address whose index corresponds to the cache line index,
that is
%
\[
  \forall \var{i} \in \texttt{Index}, \func{index}(c(i).\func{tag}) = i
\]

\paragraph{Memory Controller.}
%
The set of states of the memory controller is denoted by $\val{MC}$.
%
\[
  \texttt{MC} \triangleq \langle~\func{d\_open}: \setshortdef{\val{true},
    \val{false}},\quad\func{d\_lock}: \setshortdef{\val{true},
    \val{false}}~\rangle
\]
%
The two booleans $\func{d\_open}$ and $\func{d\_lock}$ model two bits of a
configuration register named \texttt{smramc}.
%
They are used to determine how the memory controller dispatches the \IO which
targets a physical address of the SMRAM.
%
For a memory controller state $\var{mc} \in \texttt{MC}$ to be consistent with
respect to the hardware specifications, it has to verify that
%
\[
  \var{mc}.\func{d\_lock} = \val{true} \Rightarrow \var{mc}.\func{d\_open} =
  \val{false}
\]

We model the SMRAM with two ranges of addresses:
%
\begin{itemize}
\item
  $\texttt{hSmram} \triangleq \setdef{\textsc{Dram}(i)}{\val{smram\_base} \leq i
    \le \val{smram\_end}}$ the SMRAM memory range within the DRAM memory
%
\item
  $\texttt{pSmram} \triangleq \setdef{\textsc{Pa}(i)}{\val{smram\_base} \leq i
    \le \val{smram\_end}}$ the projection of the SMRAM in the address space
  manipulated by the core
\end{itemize}
%
The values of $\val{smram\_base}$ and $\val{smram\_end}$ are specified in the
memory controller specifications and are left as a parameter of our model.
%
It is the software responsability to set the \ac{smrr} accordingly.
%
We assume $\val{smram\_end} - \val{smram\_base} > \val{0x8000}$, that is the
first instruction executed by the core after entering \ac{smm} (located at
$\texttt{SMBASE} + \val{0x8000}$) is inside the SMRAM if the \texttt{SMBASE}
register is correctly configured.

The memory controller translates physical addresses into hardware addresses and
forwards the \IO accordingly.
%
We model this translation with the function
%
\[
  \func{dispatch} : \texttt{MC} \times \setshortdef{\val{true},\,\val{false}}
  \times \texttt{PhysAddr} \rightarrow \texttt{HardAddr}
  %
\]
%
\begin{definition}
  \label{def:speccert2:dispatch}

  The function \func{dispatch} is defined as follows:
  \[
    \func{dispatch}(\var{mc}, \var{in\_smm}, \var{pa}) \triangleq
    \begin{cases}
      \textsc{Vga}(i) & \text{if }\var{in\_smm} = \val{false}\text{, }pa \in
      \texttt{pSmram}\text{,} \\
      & \quad\text{ and } \var{mc}.\func{d\_open} = \val{false}  \\
      \textsc{Dram}(i) & \text{otherwise}
    \end{cases}
  \]
%
  \qquad\qquad\qquad\qquad\qquad\qquad where $pa = \textsc{Pa}(i)$.
\end{definition}

We emphasize that the same physical address can be translated into two different
hardware addresses for two memory controller states $m$ and $m'$, hence it is
possible to have
%
\[
  \func{dispatch}(m, b, pa) \neq \func{dispatch}(m', b, pa)
\]

\paragraph{Memories.}
%
The physical memories (exposed by the DRAM and the VGA controllers)
are modelled together with a mapping between the hardware addresses and both their
contents and the software components which own them.
%
We write $\texttt{Mem}(S)$ for the set of states of the physical memories.
\[
  \texttt{Mem}(S) \triangleq \texttt{HardAddr} \rightarrow
  \langle~\func{content}: \texttt{Val},\quad\func{owner}: S~\rangle
\]

% \paragraph{}% Conclusion of the subsection
% %
% The hardware architecture states are implemented in the
% \emph{SpecCert.x86.Archi\-tecture} module (about 1\,500 lines of code).
% %
% In addition to the state definitions, we have implemented several helper
% functions and predicates.
% %
% For instance,
% %
% \begin{itemize}
% \item
%   $\func{address\_location\_owner} : H(S) \times \texttt{PhysAddr} \rightarrow
%   S$
%   \begin{quote}
%     \small Given a hardware architecture state and a physical address, returns
%     the memory content software owner
%   \end{quote}
% %
% \item $\func{cache\_hit}$, a predicate on $H(S) \times \texttt{PhysAddr}$
%   \begin{quote}
%     \small Given a hardware architecture state and a physical address, holds
%     true if the memory content is in the cache
%   \end{quote}
% %
% \item $\func{cache\_line\_owner} : H(S) \times \texttt{Index} \rightarrow S$
%   \begin{quote}
%     \small Given a hardware architecture state and a cache line index, returns
%     the owner of this cache line
%   \end{quote}
% %
% \item
%   $\func{resolve\_cache\_strategy} : H(S) \times \texttt{PhysAddr} \rightarrow
%   \texttt{CacheStrat}$
%   \begin{quote}
%     \small Given a hardware architecture state and a physical address, returns
%     the cache strategy used by the \ac{cpu} for this address
%   \end{quote}
% %
% \item
%   $\func{translate\_physical\_address} : H(S) \times \texttt{PhysAddr}
%   \rightarrow \texttt{HardAddr}$
%   \begin{quote}
%     \small Given a hardware architecture state and a physical address, returns
%     the result of the memory controller address translation
%   \end{quote}
% \end{itemize}

\subsection{Transition Labels and Transition Relation}
\label{subsec:speccert2:labtrans}

In accordance with Definition~\ref{def:speccert:model}, we consider two sets of
labels to distinguish between software transitions (direct, foreseeable
consequences of instructions execution, denoted by $L_S$) and hardware
transitions (denoted by $L_H$).
%
For each transition label we define
%
\begin{inparaenum}[(1)]
\item a precondition to determine whether the transition can occur from a given
  hardware state, and
    %
\item a postcondition to specify the consequences of the transitions over the
  hardware architecture state.
\end{inparaenum}
%
Let \func{pre} be a predicate on \( L \times H(S) \), and \func{post} be a
predicate on \( L \times H(S) \times H(S) \), then the transition relation of
{\scshape Minx86} is defined as follows:
%
\[
  h \xrightarrow[\func{context}]{l} h' \triangleq \func{pre}(l, h) \wedge
  \func{post}(l, h, h')
\]

% and~\ref{tab:speccert2:hardlab}
\begin{table}[t]
  \centerline{%
    \begin{tabular}{lp{3cm}p{6cm}}
      \hline
      \multicolumn{1}{c}{\scshape Event}
      & \multicolumn{1}{c}{\scshape Parameters}
      & \multicolumn{1}{c}{\scshape Description} \\
      \hline
      \( \func{Write} \)
      & \( \var{pa} \in \texttt{PhysAddr} \) \newline
        \( v \in \texttt{Val} \)
      & A core \IO to write at physical address \( \var{pa} \) the value \(
        v \) \\
      \hline
      \( \func{Read} \)
      & $\var{pa} \in \texttt{PhysAddr}$
      & A core \IO to read at physical address \( \var{pa} \). \\
      \hline
      \( \func{SetCacheStrat} \)
      & \( \var{pa} \in \texttt{PhysAddr} \) \newline
        \( \var{strat} \in \texttt{CacheStrat} \)
      & Change the cache strategy for \( \var{pa} \) to \( \var{strat} \) \\
      \hline
      \( \func{UpdateSmrr} \)
      & \( \var{smrr} \in \texttt{Smrr} \)
      & Update the \ac{smrr} content with the new value \( smrr \) \\
      \hline
      \( \func{Rsm} \)
      & \centering ---
      & The core leaves \ac{smm} \\
      \hline
      \( \func{OpenBitFlip} \)
      & \centering ---
      & Flip the \texttt{d\_open} bit \\
      \hline
      \( \func{LockSmramc} \)
      & \centering ---
      & Set the \texttt{d\_lock} bit \\
      \hline
      \( \func{NextInstruction} \)
      & \( \var{pa} \in \texttt{PhysAddr} \)
      & The program counter register of the core is set to \( \func{pa} \) \\
      \hline
    \end{tabular}
  }
  \caption{List of labels dedicated to {\scshape Minx86} software transitions
    (\( L_S \))}
  \label{tab:speccert2:softlab}
\end{table}

\paragraph{Software Transitions.}
%
Table~\ref{tab:speccert2:softlab} lists the labels dedicated to software
transitions in terms of constructors.
%
We model the core \IOs with \( \func{Read}(\var{pa}) \) and
\( \func{Write}(\var{pa}) \), the configuration of the memory controller with
\( \func{OpenBitFlip} \) and \( \func{LockSmramc} \), the configuration of the
cache strategy with \( \func{SetCacheStrat}(\var{pa}, \var{strat}) \), the
configuration of the \ac{smrr} with \( \func{UpdateSmrr}(\var{smrr}) \), the
exit of the \ac{smm} with $\func{Rsm}$, and the update of the core program
counter register with $\func{NextInstruction}(\var{pa})$.

% As an example, the pre and postconditions of the \func{Write} transitions are
% formally defined in Figure~\ref{fig:speccert2:softprepost}.
%%
% \TODO{Il faut commenter cette figure (et donc, ces définitions formelles)}
%%
% They are presented under the following form:
%%
% \begin{prooftree}
%   \AxiomC{\func{pre}(l, h)}%
%   \AxiomC{\func{post}(l, h, h')}%
%   \BinaryInfC{\( h \xrightarrow[\func{context}]{l} h' \)}%
% \end{prooftree}

We now give description of the pre and postconditions for each label.
%
The interested readers can refer to the Coq
development\,\cite{letan2016speccertcode} in case they want to review their
definitions.
%
% \TODO{Je trouve cette dernière phrase un peu maladroite. En outre, le code Coq
% ne fait pas parti de la thèse. Peut-être cela serait-il pertinent de mettre
% toutes les définitions formelles en annexes sous forme textuelle et rappeler
% que ces définitions sont également disponibles dans le code Coq (dans tel
% fichier). En tout cas, donner seulement un exemple de définition formelle dans
% le coeur du manuscrit me semble la bonne stratégie. Par contre, renvoyer le
% lecteur au code Coq pour les autres me semble un peu limite}

% \begin{figure}[p]
%   {\small
%   \begin{prooftree}
%     \AxiomC{%
%     \(
%     \begin{array}{l}
%          h.\func{core}.\func{strat}(\var{pa}) = \val{WB} \\
%          \wedge~\func{cache\_hit}(h.\func{cache}, \var{pa}) \\
%          \wedge~\var{pa} \not\in h.\func{smrr}.\func{range}
%        \end{array}
%        \) }%
%        \AxiomC{%
%        \( h' = h \left\{ \func{cache}(\func{index}(\var{pa})) \leftarrow
%          h.\func{cache}(\func{index}(\var{pa}))\left\{
%            \begin{array}{c}
%              \func{content} \leftarrow v, \\
%              \func{dirty} \leftarrow \val{true}, \\
%              \func{owner} \leftarrow \func{context}(h)
%            \end{array}
%          \right\} \right\} \) }%
%        \BinaryInfC{%
%        \( h \xrightarrow[\func{context}]{\func{Write}(\var{pa}, v)} h' \) }
%      \end{prooftree}
%
%    \begin{prooftree}
%      \AxiomC{%
%      \(
%      \begin{array}{l}
%          h.\func{core}.\func{strat}(\var{pa}) = \val{WB} \\
%          \wedge~\neg~\func{cache\_hit}(h.\func{cache}, \var{pa}) \\
%          \wedge~\var{pa} \not\in h.\func{smrr}.\func{range}
%        \end{array}
%        \) }%
%        \AxiomC{%
%        \(
%        \begin{array}{l}
%          \text{let } \var{ha} = \func{dispatch}(h.\func{mc}, h.\func{in\_smm},
%          \var{pa}) \\
%          \text{let } \var{i} = \func{index}(\var{pa}) \\
%          h' = h \left\{
%          \begin{array}{c}
%            \func{mem}(\var{ha})
%            \leftarrow \left<
%            \begin{array}{c}
%              \func{content} = h.\func{cache}(i).\func{content}, \\
%              \func{owner} = h.\func{cache}(i).\func{owner} \\
%            \end{array}
%            \right>, \\
%            \func{cache}(i) \leftarrow \left<
%            \begin{array}{c}
%              \func{content} = v, \func{tag} = \var{pa}, \\
%              \func{owner} = \func{context}(h), \func{dirty} = \val{true}
%            \end{array}
%            \right>
%          \end{array}
%          \right\}
%        \end{array}
%        \) }%
%        \BinaryInfC{%
%        \( h \xrightarrow[\func{context}]{\func{Write}(\var{pa}, v)} h' \) }
%      \end{prooftree}
%
%    \begin{prooftree}
%      \AxiomC{%
%      \(
%      \begin{array}{l}
%          h.\func{core}.\func{smrr}.\func{strat} = \val{WB} \\
%          \wedge~\func{cache\_hit}(h.\func{cache}, \var{pa}) \\
%          \wedge~\var{pa} \in h.\func{smrr}.\func{range} \\
%          \wedge~h.\func{core}.\func{in\_smm} = \val{true}
%        \end{array}
%        \) }%
%        \AxiomC{%
%        \( h' = h \left\{ \func{cache}(\func{index}(\var{pa})) \leftarrow
%          h.\func{cache}(\func{index}(\var{pa}))\left\{
%            \begin{array}{c}
%              \func{content} \leftarrow v, \\
%              \func{dirty} \leftarrow \val{true}, \\
%              \func{owner} \leftarrow \func{context}(h)
%            \end{array}
%          \right\} \right\} \) }%
%        \BinaryInfC{%
%        \( h \xrightarrow[\func{context}]{\func{Write}(\var{pa}, v)} h' \) }
%      \end{prooftree}
%
%    \begin{prooftree}
%      \AxiomC{%
%      \(
%      \begin{array}{l}
%          h.\func{core}.\func{smrr}.\func{strat} = \val{WB} \\
%          \wedge~\neg~\func{cache\_hit}(h.\func{cache}, \var{pa}) \\
%          \wedge~\var{pa} \in h.\func{smrr}.\func{range} \\
%          \wedge~h.\func{core}.\func{in\_smm} = \val{true}
%        \end{array}
%        \) }%
%        \AxiomC{%
%        \(
%        \begin{array}{l}
%          \text{let } \var{ha} = \func{dispatch}(h.\func{mc}, h.\func{in\_smm},
%          \var{pa}) \\
%          \text{let } \var{i} = \func{index}(\var{pa}) \\
%          h' = h \left\{
%          \begin{array}{c}
%            \func{mem}(\var{ha})
%            \leftarrow \left<
%            \begin{array}{c}
%              \func{content} = h.\func{cache}(i).\func{content}, \\
%              \func{owner} = h.\func{cache}(i).\func{owner} \\
%            \end{array}
%            \right>, \\
%            \func{cache}(i) \leftarrow \left<
%            \begin{array}{c}
%              \func{content} = v, \func{tag} = \var{pa}, \\
%              \func{owner} = \func{context}(h), \func{dirty} = \val{true}
%            \end{array}
%            \right>
%          \end{array}
%          \right\}
%        \end{array}
%        \) }%
%        \BinaryInfC{%
%        \( h \xrightarrow[\func{context}]{\func{Write}(\var{pa}, v)} h' \) }
%      \end{prooftree}
%
%    \begin{prooftree}
%      \AxiomC{%
%      \(
%      \begin{array}{l}
%          h.\func{core}.\func{strat}(\var{pa}) = \val{UC} \\
%          \wedge~\var{pa} \not\in h.\func{smrr}.\func{range}
%        \end{array}
%        \) }%
%        \AxiomC{%
%        \(
%        \begin{array}{l}
%          \text{let } \var{ha} = \func{dispatch}(h.\func{mc}, h.\func{in\_smm},
%          \var{pa}) \\
%          h' = h \left\{
%          \begin{array}{c}
%            \func{mem}(\var{ha})
%            \leftarrow \left<
%            \begin{array}{c}
%              \func{content} = v, \\
%              \func{owner} = \func{context}(h) \\
%            \end{array}
%            \right>
%          \end{array}
%          \right\}
%        \end{array}
%        \) }%
%        \BinaryInfC{%
%        \( h \xrightarrow[\func{context}]{\func{Write}(\var{pa}, v)} h' \) }
%      \end{prooftree}
%
%    \begin{prooftree}
%      \AxiomC{%
%      \(
%      \begin{array}{l}
%          h.\func{core}.\func{smrr}.\func{strat} = \val{UC} \\
%          \wedge~\var{pa} \in h.\func{smrr}.\func{range} \\
%          \wedge~h.\func{core}.\func{in\_smm} = \val{true}
%        \end{array}
%        \) }%
%        \AxiomC{%
%        \(
%        \begin{array}{l}
%          \text{let } \var{ha} = \func{dispatch}(h.\func{mc}, h.\func{in\_smm},
%          \var{pa}) \\
%          h' = h \left\{
%          \begin{array}{c}
%            \func{mem}(\var{ha})
%            \leftarrow \left<
%            \begin{array}{c}
%              \func{content} = v, \\
%              \func{owner} = \func{context}(h) \\
%            \end{array}
%            \right>
%          \end{array}
%          \right\}
%        \end{array}
%        \) }%
%        \BinaryInfC{%
%        \( h \xrightarrow[\func{context}]{\func{Write}(\var{pa}, v)} h' \) }
%      \end{prooftree}
%
%    \begin{prooftree}
%      \AxiomC{%
%      \(
%      \begin{array}{l}
%          \var{pa} \in h.\func{smrr}.\func{range} \\
%          \wedge~h.\func{core}.\func{in\_smm} = \val{false}
%        \end{array}
%        \) }%
%        \AxiomC{%
%        \( h' = h \) }%
%        \BinaryInfC{%
%        \( h \xrightarrow[\func{context}]{\func{Write}(\var{pa}, v)} h' \) }
%      \end{prooftree}
%    }
%
%      \caption{Pre and postconditions for {\scshape Minx86} \( \func{Write} \)
%      transitions}
%      \label{fig:speccert2:softprepost}
%    \end{figure}

Because the pagination and segmentation mechanisms are outside of the current
scope of {\scshape Minx86}, we consider that a core can always read and write at
any physical address.
%
As a consequence, the precondition for \( \func{Read}(\var{pa}) \) and
\( \func{Write}(\var{pa}) \) always holds true.
%
The postconditions for \( \func{Read}(\var{pa}) \) and
\( \func{Write}(\var{pa}, v) \) take into account
%
\begin{inparaenum}[(1)]
\item the cache strategy configured for the address \( pa \),
\item the state of the cache line associated to the index
  \( i = \func{index}(pa) \),
\item whether \( pa \) belongs to the SMRAM or not,
\item whether the core is in \ac{smm} or not, and
\item whether the \texttt{D\_OPEN} bit of the \texttt{SMRAMC} register is set or
  not,
\end{inparaenum}
%
in order to determine which memory locations are updated and modify their
respective owner accordingly.
%
We discuss two specific cases of a transition labeled
\( \func{Write}(v, \func{pa}) \) from a state \( h \) to a state \( h' \) in
order to illustrate the definition of the postcondition predicate:
%
\begin{enumerate}
\item If the cache strategy for the address \( \var{pa} \) is set to
  \( \mathtt{WB} \), the content of this address is not currently cached by the
  processor, the cache line indexed by \( i = \func{index}(pa) \) is marked as
  ``dirty'', and \( \var{pa} \) does not fall in the range specified by the core
  SMRR, that is
  %
  \[
    h.\func{core}.\func{strat}(\var{pa}) = \mathtt{WB} \wedge \neg
    \func{cache\_miss}(h.\func{cache}, \var{pa}) \wedge \var{pa} \not\in
    h.\func{smrr}.\func{range} \wedge h.\func{cache}(i).\func{dirty} =
    \mathtt{true}
  \]
  %
  then the content of the cache line is written back to memory, and the cache
  line is updated with \( v \) as its new content, \( context(h) \) as its new
  owner, \( pa \) as its new tag and its dirty bit is set.
  %
  For this specific case, with
  \( \var{ha} = \func{dispatch}(h.\func{mc}, h.\func{in\_smm},
  h.\func{cache}(i).\func{tag}) \), the postcondition is
  %
  \[
    \begin{array}{l}
      h' = h \left\{
      \begin{array}{c}
        \func{mem}(\var{ha})
        \leftarrow \left<
        \begin{array}{c}
          \func{content} = h.\func{cache}(i).\func{content}, \\
          \func{owner} = h.\func{cache}(i).\func{owner} \\
        \end{array}
        \right>, \\
        \func{cache}(i) \leftarrow \left<
        \begin{array}{c}
          \func{content} = v, \func{tag} = \var{pa}, \\
          \func{owner} = \func{context}(h), \func{dirty} = \val{true}
        \end{array}
        \right>
      \end{array}
      \right\}
    \end{array}
  \]
  %
  Note that the same scenario applies if \( \var{pa} \) belongs to the range
  specified by the core SMRR, and the cache strategy specified by the SMRR is
  \( \mathtt{WB} \).
  %
\item If we consider a similar case, but this time the address \( \var{pa} \) is
  already present inside the cache, then only the cache is updated.
  %
  The new owner of the selected cache line is the software component currently
  executed by the core, its new content is \( v \) and its dirty bit is set,
  while the tag of the cache line remains untouched.
  %
  In this second scenario, the postcondition is
  %
  \[
    \begin{array}{l}
      h' = h \left\{
      \begin{array}{c}
        \func{cache}(i) \leftarrow h.\func{cache}(i) \left\{
        \begin{array}{c}
          \func{content} = v, \\
          \func{owner} = \func{context}(h),\\
          \func{dirty} = \val{true}
        \end{array}
        \right\}
      \end{array}
      \right\}
    \end{array}
  \]
\end{enumerate}

A software component can always update the cache strategy used for an \IO.
%
The postcondition for \( \func{SetCacheStrat}(\var{pa}, \func{strat}) \)
requires only the cache strategy setting for this physical address $\var{pa}$ to
change. %, that is
%
%\[
%  h' = h \left\{ \func{core}.\func{strat}(\var{pa}) \leftarrow \var{strat}
%  \right\}
%\]
%
The precondition for \( \func{UpdateSmrr} \) requires the core to be in
\ac{smm}. %, that is
%
%\[
%  h.\func{core}.\func{in\_smm} = \val{true}
%\]
%
The postcondition requires the \ac{smrr} of the core to be updated with the
correct value, the rest of the hardware architecture state being left unchanged.

A software component can jump to any physical address, that is the preconditions
for the label \( \func{NextInstruction}(\var{pa}) \) always hold true.
%
The postcondition for \( \func{NextInstruction}(\var{pa}) \) requires the
program counter register to be updated with \( \var{pa} \).
%
The \( \func{OpenBitFlip} \) precondition requires the \texttt{SMRAMC} register
to be unlocked.
%
The postcondition requires the \texttt{d\_open} bit to be updated.
%
The $\func{LockSmramc}$ precondition requires the SMRAM to be unlocked,
\emph{i.e.} the \texttt{d\_lock} bit to be unset.
%
The postcondition requires the $\texttt{d\_open}$ bit to be unset and the
\texttt{d\_lock} bit to be unset.

\begin{table}[t]
  \bigcentering
  \begin{tabular}{lp{9cm}}
    \hline
    \multicolumn{1}{c}{\scshape Event}
    & \multicolumn{1}{c}{\scshape Description} \\
    \hline
    \(\func{ReceiveSMI}\)
    & A \ac{smi} is raised and the core enters \ac{smm} \\
    \hline
    \(\func{Fetch}\)
    & A core \IO to fetch the instruction stored at the physical address
      contained in the program counter register \\
    \hline
  \end{tabular}
  \caption{List of labels dedicated to {\scshape Minx86} hardware transitions
    (\( L_H \))}
  \label{tab:speccert2:hardlab}
\end{table}

\paragraph{Hardware Transitions.}
Table~\ref{tab:speccert2:hardlab} lists the labels dedicated to hardware
transitions in terms of constructors.

\begin{figure}
  \centering
  \begin{prooftree}
    \AxiomC{\( h.\func{core}.\func{in\_smm} = \val{false} \)}%
    \AxiomC{\( h' = h \left\{
        \begin{array}{c}
          \func{core}.\func{in\_smm} \leftarrow \val{true}, \\
          \func{core}.\func{pc} \leftarrow h.\func{core}.\func{smbase} + \val{0x8000}
        \end{array}
      \right\} \)}%
    \BinaryInfC{\( h \xrightarrow[\func{context}]{\func{ReceiveSMI}} h' \)}
  \end{prooftree}

  \caption{Pre and postconditions for {\scshape Minx86} \func{ReceiveSMI}
    transitions}
  \label{fig:speccert2:receiveSMI}
\end{figure}

\func{ReceiveSMI} models a \ac{smi} being raised and handled by the core.
%
Its precondition requires the core not to be in \ac{smm} because \ac{smm} is
non-reentrant.
%
The postcondition of \func{ReceiveSMI} requires the program counter to be set to
$\texttt{SMBASE} + \val{0x8000}$ and the core enters in \ac{smm}.
%
Both the pre and postcondition of \func{ReceiveSMI} are formally defined in
Figure~\ref{fig:speccert2:receiveSMI}.
%
$\func{Fetch}$ models the \IO to fetch the instruction pointed by the program
counter register.
%
As a consequence, the definition of its precondition and postcondition are the
same as \( \func{Read}(h.\func{core}.\func{pc}) \).
%
% We distinguish between \func{Read} and \func{Fetch} because a software
% component does not always decide the value of the program counter register of
% the Core.
%%
% \TODO{Je comprends la fin de la phrase mais je ne vois pas le lien avec "We
% distinguish between \func{Read} and \func{Fetch}". Difficile de comprendre ce
% que tu voulais dire.}
%%
% For instance, the value of the program counter is updated during a
% \func{ReceiveSMI} transition.

\subsection{Transition-Software Mapping}
\label{subsec:minx86:fetched}

We define \(\textsc{Minx86}\func{\_fetched}\) a transition-software mapping for
$\formatLTS{Minx86}$ (Definition~\ref{def:speccert:transsoft}), to map an
initial state of a transition and the label of this transition to the set of
software components which own an instruction fetched during this transition.
%
In the case of \formatLTS{Minx86}, there is only one event which implies
fetching instructions: \func{Fetch}.
%
To define the transition-software mapping in this specific case, we rely on a
helper function
\( \func{read\_from\_cache} : H(S) \times \texttt{PhysAddr} \rightarrow \{
\texttt{true}, \texttt{false} \} \) which returns \texttt{true} if a core will
read the content of a given physical address from the cache, and \texttt{false}
otherwise.
%
This function is defined as follows:
%
\[
  \func{read\_from\_cache}(h, \var{pa}) \triangleq \left\{
    \begin{array}{ll}
      \texttt{true} & \text{if }\var{pa} \not\in
                      h.\func{core}.\func{smrr}.\func{range}\text{, }
                      h.\func{core}.\func{strat} = \texttt{WB} \\
                    & \quad\text{and } \func{cache\_hit}(h.cache, \var{pa}) \\
      \texttt{true} & \text{if }\var{pa} \in
                      h.\func{core}.\func{smrr}.\func{range}\text{, }
                      h.\func{core}.\func{smrr}.\func{strat} = \texttt{WB}, \\
                    & \quad \func{cache\_hit}(h.cache, \var{pa}) \text{ and } h.\func{core}.\func{in\_smm} =
                      \texttt{true} \\
      \texttt{false} & \text{otherwise}
    \end{array}
  \right.
\]
%
Using the function \func{read\_from\_cache}, we can define the
transition-software mapping of {\scshape Minx86} as follows:
%
\[
  \textsc{Minx86}\func{\_fetched}(h, l) = \left\{
    \begin{array}{ll}
      \emptyset & \text{if } l \neq \func{Fetch} \\
      \emptyset & \text{if } h.\func{core}.\func{pc} \in
                  h.\func{core}.\func{smrr}.\func{range} \\
                & \quad \text{ and }
                  h.\func{core}.\func{in\_smm} = \texttt{false} \\
      \{ h.\func{cache}(\func{index}(\var{pc})).\func{owner} \} & \text{where }
                                                                  \var{pc} =
                                                                  h.\func{core}.\func{pc}
                                                                  \text{ and} \\
                & \quad\text{if }\func{read\_from\_cache}(h, \var{pc})
                  = \texttt{true} \\
      \{ h.\func{mem}(\var{ha}).\func{owner} \} & \text{where } \\
                & \quad\var{ha} =
                  \func{dispatch}(h.\func{mc},
                  h.\func{in\_smm}, h.\func{core}.\func{pa})

    \end{array}
  \right.
\]
%
We consider four cases.
%
Firstly, an instruction is fetched during a \func{Fetch} transition only, so for
other transitions, the function returns the empty set.
%
Secondly, if a core not in \ac{smm} tries to read the content of a physical
addresses which belongs to the range specified by its SMRR, the specification
states that the result of this \IO is a predefined value, hence the function
returns also the empty set in this case.
%
For the two last cases, we based our decision on the \func{read\_from\_cache}
function to determine the content of the singleton returned by
\( \textsc{Minx86}\func{\_fetched} \).

\subsection*{}

In this Section, we have defined a minimal x86 hardware model we called
{\scshape Minx86}.
%
In its current state, the scope of {\scshape Minx86} is limited to hardware
features involved in the \ac{hse} mechanism implemented by the \ac{bios} to
remain isolated from the rest of the software stack at runtime.
%
However, we have defined {\scshape Minx86} with extensibility in mind.
%
Notably, the hardware-software mapping used by the model in order to track
memory location ownership is left as a parameter, and {\scshape Minx86} could be
extended to reason about other \ac{hse} mechanisms whose purpose is to enforce a
code injection policy.

\section{Specifying and Verifying a BIOS HSE Mechanism}
\label{sec:speccert2:verif}

We consider the execution of the software stack made of a \ac{bios}, an
operating system and \( n \) applications, after the end of the boot sequence.
%
In this Section, our objective is to implement the three-step methodology that
we have introduced in Chapter~\ref{chapter:speccert}, that is
%
\begin{inparaenum}[(1)]
\item specifying the software requirements that must be satisfied by the trusted
  software components which implement the \ac{hse} mechanism,
%
\item specifying the targeted security policy the \ac{hse} mechanism supposedly
  enforces, and
%
\item verifying that the \ac{hse} mechanism is correct with respect to the
  targeted security policy.
\end{inparaenum}

Because the targeted security policy has already been defined previously
(Definition~\ref{def:speccert:biospol}), it leaves steps (1) and (3) to
complete.
%
We first give a formal definition of the \ac{hse} mechanism implemented by the
\ac{bios} (denoted by \( \Delta_{\mathtt{bios}} \)) to remain isolated from the
rest of the software stack (\ref{subsec:speccert2:bioshsedef}).
%
Then, we verify its correctness with respect to the \ac{bios} code injection
sub-policy \( I_{\mathtt{bios}} \) (Definition~\ref{def:speccert:biospol})
(\ref{subsec:speccert2:bioshsecorrect}).

\subsection{BIOS HSE Definition}
\label{subsec:speccert2:bioshsedef}

{\scshape Minx86} is parameterized with a hardware-software mapping
\func{context} (Definition~\ref{def:speccert:hardsoft}).
%
This mapping is necessary for the model to track memory ownership during
\func{Write}, \func{Read} and \func{Fetch} transitions.
%
Because {\scshape Minx86} is not precise enough to distinguish between the
execution of the operating system and the applications, we axiomatize the
definition of \func{context} with two rules:
%
\begin{description}
\item [\func{context}-bios:] If the core is in \ac{smm}, it executes the
  \ac{bios}
  \[
    h.\func{in\_smm} = \val{true} \Rightarrow \func{context}(h) = \mathtt{bios}
  \]
\item [\func{context}-untrusted:] If the core is not in \ac{smm}, it executes
  another component of the software stack
  \[
    h.\func{in\_smm} = \val{false} \Rightarrow \func{context}(h) \in
    \setshortdef{\mathtt{os}, \mathtt{app}_1, \dots, \mathtt{app}_n}
  \]
\end{description}

% For the sake of readability, we abusively write
% \( \func{context}(h) = \mathtt{untrusted} \) when the core executes a software
% component which is not the \ac{bios}.

We define $\Delta_{\mathtt{bios}}$ to model the \ac{hse} mechanism applied by
the \ac{bios}, such that
\[
  \Delta_{\mathtt{bios}} = \langle S, \setshortdef{\mathtt{bios}},
  \func{context}, \func{hardware\_req}, \func{bios\_req} \rangle
\]
%
As \func{context} has already been specified, only \func{hardware\_req} and
\func{bios\_req} remain.
%
Once we have properly introduced both, we verify our definition of
\( \Delta_{\mathtt{bios}} \) satisfies the \ac{hse} laws
(Definition~\ref{def:speccert:laws}).

\paragraph{Requirements over States.}
%
Given \( h \in H(S) \), we have identified six requirements over states for
\( h \) to satisfy \func{hardware\_req}.
%
\begin{enumerate}
\item When the core executes the \ac{smm} code, the program counter register
  value needs to be an address in SMRAM.
  \[
    \func{context}(h) = \mathtt{bios} \Rightarrow h.\func{core}.\func{pc} \in
    \texttt{pSmram}
  \]
  %
\item The \texttt{SMBASE} register was correctly set during the boot sequence to
  point to the base of the SMRAM.
  \[
    h.\func{core}.\func{smbase} = \textsc{Pa}(\val{smram\_base})
  \]
  %
\item All the memory locations within the SMRAM are owned by the \ac{bios}.
  \[
    \forall \var{ha} \in \texttt{hSmram}, h.\func{mem}(\var{ha}).\func{owner} =
    \mathtt{bios}
  \]
  %
\item For a physical address in SMRAM, in case of cache hit, the related cache
  line content must be owned by the \ac{smm} code.
  \[
    \forall \var{pa} \in \texttt{pSmram}, \func{cache\_hit}(h.\func{cache},
    \var{pa}) \Rightarrow h.\func{cache}(\func{index}(\var{pa})).\func{owner} =
    \mathtt{bios}
  \]
  %
\item In order to protect the content of the SMRAM inside the DRAM memory, the
  boot sequence code has locked the SMRAMC controller.
  %
  This ensures that an OS cannot set the \texttt{d\_open} bit any longer and
  only a core in \ac{smm} can modify the content of the SMRAM.
  \[
    h.\func{mc}.\func{d\_lock} = \val{true}
  \]
  %
\item The range of memory declared with the \ac{smrr} needs to overlap with the
  SMRAM.
  \[
    \texttt{pSmram} \subseteq h.\func{core}.\func{smrr}.\func{range}
  \]
\end{enumerate}

We do not consider the boot sequence in this verification problem and the
\ac{bios} must correctly configure the hardware architecture prior to start the
execution of the operating system for the initial state of the considered trace
to satisfy our 6 requirements over states.
%
Otherwise, the trace will not comply with \( \Delta_{\mathtt{bios}} \), and
there is no guarantee that the security policy will be enforced, independently
from the correctness of \( \Delta_{\mathtt{bios}} \).

% \TODO{A la lecture de ces requirements, je m'aperçois que certains d'entre eux
% s'apppuient sur une configuration réalisée au préalable par le code trusté,
% durant la phase de boot. On peut donc dire que les exigences sur le code
% trusté ne sont pas seulement dans les software requirements mais aussi, pour
% certaines, dans les hardware requirements (et du coup le nom n'est peut être
% pas idéal), même si elles ne correspondent pas à la même phase d'utilisation
% (initialisation vs. runtime en quelque sorte)}

\paragraph{Requirements over Transitions.}
%
We now define $\func{bios\_req}$.
%
We only define two restrictions.
%
First, we force the \ac{bios} execution to remain confined within the SMRAM.
%
The reason is simple: the OS can tamper with the memory outside the SMRAM.
%
As a consequence, jumping outside the SMRAM is the best way to fail the security
policy.
%
Secondly, we prevent the \ac{bios} to update the \ac{smrr} registers;
%
the latter should have been properly configured during the boot sequence, and
there is no need to further update them.
%
\[
  \begin{array}{rcl}
    \func{bios\_req}(h, l)
    & \triangleq
    & \func{context}(h) = \val{bios} \\
    % no func name
    & %no triangleq
    & \Rightarrow ((\forall \var{pa} \in \texttt{PhysAddr}, \\
    % no func name
    & %no triangleq
    & \qquad\qquad l = \func{NextInstruction}(\var{pa})
      \Rightarrow \var{pa} \in \texttt{pSmram}) \\
      % no func name
    & %no triangleq
    & \qquad\wedge~(\forall \var{smrr} \in \texttt{Smrr}, l \neq \func{UpdateSmrr}(\var{smrr})))
  \end{array}
\]

\paragraph{Verifying the HSE Laws.}
%
For $\Delta_{\mathtt{bios}}$ to be a \ac{hse} mechanism, we need to prove it
satisfies the two \ac{hse} laws.

The first law states that the \( \func{bios\_req} \) is always satisfied when a
non-trusted software component, i.e. different from the \ac{bios}, is executed
by the hardware architecture.
%
By definition of \func{bios\_req}, \( \func{context}(h) = \mathtt{bios} \) is an
antecedent of the requirements over \func{NextInstruction} and
\func{UpdateSmrr}.
%
Therefore, the first \ac{hse} law is satisfied for \( \Delta_{\mathtt{bios}} \).

The second law requires the state requirements to be invariant with respect to
the software requirements.
%
We prove this by case enumeration of \( l \in L_S \uplus L_H \) and
\( h \in H(S) \), we check that each requirement described previously is
preserved by \func{bios\_req}.
%
In practice, these proofs turned out to be the more demanding, especially for
requirement 3. ---the content of the SMRAM is owned by the \ac{bios}--- and
4. ---the cache line tagged with physical addresses related to the SMRAM are
owned by the \ac{bios}--- because this requires to take the SMRR and the
write-back strategy into account for the \func{Write} and \func{Read}.
%
We take the example of \func{Read} transitions; a similar approach applies to
\func{Write} transitions.

Given a transition labeled \( \func{Read}(\var{pa}) \) from \( h \) to \( h' \),
there are three case scenarios to consider:
%
\begin{enumerate}
\item The core discards the read, as part of the SMRAM cache poisoning
  countermeasure.
  %
  In this context, the hardware architecture is not updated, therefore
  \( h = h' \) and we can conclude that
  \( \func{hardware\_req}(h) \Rightarrow \func{hardware\_req}(h') \).
\item The core is configured to read the content of \( pa \) from the underlying
  memories (uncacheable strategy).
  %
  Thanks to requirements 5., we know that the \texttt{SMRAMC} register has been
  correctly configured, so by definition of \func{dispatch}
  (Definition~\ref{def:speccert2:dispatch}), we know that only a core in
  \ac{smm} can update the content of the SMRAM.
  %
  We conclude that the content of the SMRAM remains owned by the \ac{bios} in
  \( h' \).
\item The core is configured to read the content of \( pa \) from the cache, and
  may have to perform a cache eviction if necessary (write-back strategy).
  %
  This last case is the more complex to consider, because we have to consider:
  %
  whether the core is in \ac{smm} or not;
  %
  whether the targeted physical address falls into the \ac{smrr} range or not;
  %
  whether the \IO results in a cache hit or not;
  %
  in case of cache miss, whether the occupied cache line is tagged ``dirty'' or
  not;
  %
  in case of a dirty bit, whether its tag (a physical address) belongs to the
  SMRAM or not.
  %
  Taken separately, each case is relatively straightforward to prove.
  %
  For instance, in case of cache miss, but the occupied cache line is not tagged
  as ``dirty'', then the underlying memories are not updated, so we can conclude
  that the SMRAM remains owned by the \ac{bios}.
  %
  To manage the number of cases to consider, we carefully organize our proofs in
  general-purpose lemmas which applies to both \func{Read} and \func{Write}
  transitions\,\footnote{The interesting reader can have a look at the file
    \texttt{src/Smm/Delta/Preserve/Architecture.v}
    in~\ref{letan2016speccertcode}, which gather these proofs, and
    \texttt{src/Smm/Delta/Preserve/\{Read,Write\}.v} to see them in action.}.
\end{enumerate}
%
Once each requirement has been shown to be preserved during a transition, we can
conclude for \func{hardware\_req} as a whole.

\subsection{BIOS HSE Mechanism Correctness}
\label{subsec:speccert2:bioshsecorrect}

Our objective is to prove that \( \Delta_{\mathtt{bios}} \) is correct with
respect to \( I_{\mathtt{bios}} \).
%
Because \( I_{\mathtt{bios}} \) is a security policy modelled with a predicate
on transitions, we know thanks to Theorem~\ref{theorem:speccert:correcthse} that
%
\[
  \begin{array}{l}
    \forall (h, l, h') \in \mathcal{T}(\textsc{Minx86}(\func{context})), \\
    \qquad (\func{hardware\_req}(h)\ \wedge\ (l \in L_S \Rightarrow
    \func{bios\_req}(h, l))) \Rightarrow I_{\mathtt{bios}}(h, l, h')
  \end{array}
\]
%
is a sufficient condition for
\( \Delta_{\mathtt{bios}} \models I_{\mathtt{bios}} \).

From the security policy perspective, only the \func{Fetch} label is relevant.
%
Indeed, a code injection can only occur when an instruction owned by a software
components is fetched in order to be executed by a core
(Definition~\ref{subsec:speccert:tampering}).
%
For {\scshape Minx86}, this only happens during transitions labeled by
\func{Fetch} by definition of \( \textsc{Minx86}\func{\_fetched} \) (see
Subsection~\ref{subsec:minx86:fetched}).
%
\( \func{Fetch} \) is a hardware transition, and therefore is not concerned by
\func{bios\_req}.
%
As a consequence, we can simplify our proof goal as follows:
%
\[
  \begin{array}{l}
    \forall (h, h') \in H(S) \times H(S) \text{ such that } h
    \xrightarrow[\func{context}]{\func{Fetch}} h', \\
    \qquad \func{hardware\_req}(h)\ \Rightarrow I_{\mathtt{bios}}(h, \func{Fetch}, h')
  \end{array}
\]
%
We now unfold the definition of \( I_{\mathtt{bios}} \)
(Definition~\ref{def:speccert:biospol}) and of the code injection
(Definition~\ref{def:speccert:tempering}), which allows us to simplify further
our proof goal as follows:
%
\[
  \begin{array}{l}
    \forall (h, h') \in H(S) \times H(S) \text{ such that } h
    \xrightarrow[\func{context}]{\func{Fetch}} h', \\
    \qquad (\func{hardware\_req}(h) \wedge \func{context}(h) = \mathtt{bios})
    \Rightarrow \textsc{Minx86}\func{\_fetched}(h, \func{Fetch}) =
    \setshortdef{\mathtt{bios}}
  \end{array}
\]
%
The rest of the proof leverages the definition of \func{hardware\_req}.
%
Because of requirements 1., we know that
\( h.\func{core}.\func{pc} belongs to the \texttt{pSmram} \).
%
By definition of \( \textsc{Minx86}\func{\_fetched} \), we know that we have two
cases to consider:
%
\begin{itemize}
\item The instruction is read from the cache, and therefore
  %
  \[
    \textsc{Minx86}\func{\_fetched}(h, \func{Fetch}) =
    \setshortdef{h.\func{cache}(\func{index}(h.\func{core}.\func{pc})).\func{owner}}
  \]
  %
  In this context, we can conclude thanks to the requirements 4.
\item Otherwise, the instruction is read from the underlying memories.
  %
  More precisely, by definition \func{dispatch}
  (Definition~\ref{def:speccert2:dispatch}), we know the instruction is read
  from the \ac{dram}, and as a consequence the requirements 3. allows us to
  conclude.
\end{itemize}

As a side note, we emphasize that these proofs validate the SMRAM cache
poisoning countermeasure.
%
Without the \acp{smrr}, it is not possible to conclude about the correctness of
\( \Delta_{\mathtt{bios}} \) if the hardware model takes the cache into account.
%
We consider two scenarios.
%
On the one hand, the \ac{hse} mechanism definition takes the cache into account
too.
%
In these conditions, it is not possible to prove it satisfies the second law.
%
Indeed, the cache poisoning attack violates the requirement 4.
%
On the other hand, the \ac{hse} mechanism definition does not take the cache
into account.
%
As a consequence, it is possible to prove it satisfies the second law.
%
However, it is not possible to prove it is correct with respect to
$I_{\mathtt{bios}}$.
%
Indeed, we cannot discard the case where a cache line tagged with a physical
address which belongs to the SMRAM is owned by an untrusted software component.
%
In this scenario, we could even exhibit a trace which complies the \ac{hse}
mechanism, but does not satisfies the security policy.

\section{Conclusion}
\label{sec:speccert:discuss}

In this Chapter, we have proceeded with the case study introduced in
Section~\ref{sec:speccert:casestudy}.
%
We introduced {\scshape Minx86}, a minimal x86 hardware model conceived with
extensibility in mind.
%
We specified the \ac{hse} mechanism implemented by the \ac{bios} in order to
remain isolated from the rest of the software stack, then verified that this
mechanism is correct with respect to the so-called \ac{bios} code injection
sub-policy.
%
In summary, we have applied our three-step methodology.
%
The resulting model assumes as little as possible about the actual
implementation of the \ac{bios}, and constitutes, to the extent of our
knowledge, the first formalization of the \ac{bios} security model at runtime.
%
As we have already mentioned, our proofs have been implemented within the Coq
theorem prover to increase our confidence in our results, and have been released
as free software\,\cite{letan2016speccertcode}.
%
The resulting project counts around 4\,500 lines of code, divided into 2\,000
lines of code of definition and specification and 2\,500 lines of code of
proofs.
%
We believe this represents a manageable size regarding the complexity of the
model, but whether our approach remains applicable for a large-scale models
remained to be proven when we first released this work in 2016.
%
Subsequently, we have experimented increasing the scope of {\scshape Minx86}
with pagination, and have been convinced that considering the whole architecture
at once ---as it is done in {\scshape Minx86}--- would not scale properly.

The next Part of this thesis focuses on this problematic.
%
We propose another approach to enable modular verification of complex systems
composed of interconnected components, as hardware architectures definitely are.
