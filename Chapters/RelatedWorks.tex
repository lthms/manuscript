\chapter{State of the Art of Formal Verification}
\label{chapter:relatedwork}

\endquote{``\emph{Walking on water and developing software from a specification
    are easy if both are frozen.}''

  \hfill\footnotesize --- Edward V. Berard}

\vspace{1cm}\noindent
%
Our interest in formally verify hardware architecture in order to improve their
security is in line with an ongoing effort.
%
In 2011, Nachiketh Potlapally\,\cite{potlapally2011hardwaresecurity}, who was
working at Intel at the time, suggested that formal methods for security
validation is one of the possible solutions to uncover architectural attacks
ahead of time.
%
In 2016, Stephen Chong \emph{et al.} cite ``Hardware Architectures'' as area of
interest regarding the use of formal methods for
security\,\cite{chong2016report}.

The rest of this Chapter proceeds as follows.
%
First, we give primer on formal verification of hardware or software systems,
with a focus on security (Section~\ref{sec:related:review}).
%
Then, we focus on design methods which can be leveraged to ease the verification
process, in particular in regards to the model scale
(Section~\ref{sec:related:ease}).

\section{Formal Verification for Security}
\label{sec:related:review}

Formal verification consists in rigorously proving the correctness of a system
with respect to certain properties.
%
It is achieved \emph{via} the use of formal methods to exhibit a proof on an
abstract model (\ref{subsec:state:reason}).
%
Therefore, the formal definition of targeted security properties against the
model is therefore an important step of the process (\ref{subsec:state:secu}).
%
Many approaches have been proposed for formally verifying systems of varying
sizes and natures (\ref{subsec:state:approaches}).

\subsection{Reasoning about Systems}
\label{subsec:state:reason}

We are interested in hardware or software systems primarily identified by
%
\begin{inparaenum}[(1)]
\item their mutable state, and
%
\item their interface, that is a set of operations available for external
  components to use.
\end{inparaenum}
%
For instance, the state of a \ac{cpu} is characterized by the value of its
internal registers, while its interface is a instructions set.
%
Modelling such a system $C$ is usually done by defining a \ac{lts}
$M \triangleq \langle S, L, \rightarrow \rangle$.
%
$S$ is a set of abstract states which describe the concrete states of $C$.
%
$L$ is a set of labels to distinguish between transitions caused by different
operations of $C$.
%
Finally, $\rightarrow$ is a transition relation, such that for
$(p, q) \in S \times S$ and $l \in L$,
%
\[ p \xrightarrow{l} q \]
%
means it is possible for the system whose state corresponds to $p$ to shift in a
new state which corresponds to $q$ if the operation $l$ is invoked.

The structure of $M$ is given as an example, and its exact definition often vary
from one formalism to another.
%
In the meantime, the underlying principles remain the same.

\subsection{Specifying Security Properties}
\label{subsec:state:secu}

Once a system $C$ is modelled with a model $M$, we can reason about the
correctness of $C$ with respect to certain properties, by formalizing these
properties as a statement about $M$, then we exhibit a proof that this statement
is true.
%
This statement is based on the system executions, modelled as sequences of
transitions of $M$.
%
We denote $\Sigma(M)$ the set of all the possible executions of $C$, as modelled
in $M$.

\paragraph{Properties and Hyperproperties}
%
Certain security properties can be modelled \emph{via} a predicate $P$ on
executions.
%
In such a condition, the statement describing the security property is of the
forms
%
\[
  \forall \rho \in \Sigma(M), P(\rho)
\]
%
It is the case of safety properties (nothing ``bad'' happens) and liveness
properties (something ``good'' eventually happens).
%
For safety properties, a common approach is to distinguish between secure states
and insecure states, secure transitions and insecure transitions, and to require
that for an execution which starts from a secure state put the system in an
insecure state and no insecure transition occurs.
%
For instance, the BIOS Integrity security property
(Definition~\ref{def:usecase:biosint}) is a safety property security property.
%
As for liveness properties, they usually require to use a temporal logic which
provides a modal operator to reason about ``eventually''.
%
For instance, the BIOS Availability security property
(Definition~\ref{def:usecase:biosav}) is a liveness property

Other security (hyper)properties cannot be defined using such a predicate, and
requires to consider the set of executions as a whole. As a consequence, the
security statement is of the form
executions\,\cite{clarkson2010hyperproperties}, that is
%
\[
  P(\Sigma(M))
\]
%
A notable security property whose definition requires to consider couple of
executions is noninterference\,\cite{goguen1982security} of information flow
analysis.
%
A system enforces noninterference if its observable behaviour does not depends
on secret inputs, which informally translates to ``a system enforces
noninterference if, given two arbitrary executions with the same public inputs,
the system's observable behaviour remains the same''.

\paragraph{Modelling for Security}
%
The definition of a security property might be based on information that are not
available to the system $C$.
%
For instance, a x86 hardware architecture does not track information flows, but
it is supposed to enforce the BIOS Integrity security property, whose definition
requires to be able to tell which software components has been the source of the
result of a read access by the SMM code.
%
As a consequence, if we want to verify the x86 hardware architecture correctness
with respect to the BIOS Integrity security property, we need a ``information
flow aware'' model.

This raises questions about model reusability.
%
The x86 hardware architecture includes many different \ac{hse} mechanisms.
%
Defining one model for each is not an option, yet the security properties they
aim to enforce may heavily differ in terms of model requirements.

\subsection{Verification Approaches}
\label{subsec:state:approaches}

The last step of the formal verification is exhibiting a proof.
%
There are two main approaches to achieve this: model checking and theorem
proving.
%
For each approach, we give a small overview of its core principles.
%
In addition, we illustrate their verification process \emph{via} a common,
minimal example: an airlock system.
%
A airlock system is a device made of two doors, and an intermediary chamber.
%
To pass through the airlock system, it is necessary to open the first door,
enter the chamber, wait for the first door to close, then open the second door.
%
The security property of a airlock system is a safety property: at any time,
there is at least one door closed.

\paragraph{Model Checking}
%
Model checkers exhaustively and automatically explore a transition system in
order to verify it satisfies a specified properties.
%
These properties are defined it terms of predicate on the system's executions,
usually thanks to a temporal logic.
%
A temporal logic includes a set of modal operators, such as \emph{globally} (the
property is always true), or \emph{eventually} (the property becomes true in at
one point in the future).
%
The main advantage of model checkers is automation.
%
They take a model and a property as input, and output a result: a confirmation
that the property is verified, or a particular execution which does not satisfy
the property.
%
This execution is called a counterexample.
%
From a security perspective, a counterexample is very interesting, because it
potentially describes an attack path.

\begin{example}[Airlock System Verification in NuSMV]
  TODO.

  \inputminted[frame=single]{coq}{Listings/Airlock1.nusmv}
  \inputminted[frame=single]{coq}{Listings/Airlock-out.nusmv}

  An important feature of model checkers is that they generate a counter-example
  when they find a sequence of transitions which does not satisfy the
  specification.
  %
  For instance, if we slightly modify the NuSMV model of our airlock system so
  that it is no longer require for the second door to be close in order to open
  the first door, then the model checker generates the following
  counter-example:

  \inputminted[frame=single]{coq}{Listings/Airlock-counter.nusmv}
\end{example}

\paragraph{Theorem Proving}
%
A theorem prover is a computer program to write machine-checked proofs in a
particular formal logic, that is a formal language, to express statement, a set
of axioms, and a collection of inference rules to derive new formulas from the
axioms.
%
A theorem prover' underlying logic is more expressive than what model checkers
offer, but they lack automation for nontrivial problems.
%
They do not excuse users from \emph{writing proofs}, which can be very important
in terms of effort for real world systems.

\begin{example}[Airlock System Verification in Coq]
  The Coq theorem prover includes two languages: a rich functional language to
  write specifications ({\sc Gallina}) and a tactic language to write proofs
  ({\sc Ltac}).
  %
  We can {\sc Gallina} to describe the system in terms of states and actions to
  update this state.

  \inputminted[frame=single]{coq}{Listings/Airlock1.v}

  This reads as follows: a door is either open or close; a airlock system is
  made of two doors; we consider four actions which imply an update of the
  airlock system's state (opening the first or second door, closing the first or
  second door).

  {\sc Gallina} includes a special type, called \texttt{Prop}, to write logical
  propositions which can be either true or false.
  %
  We can define the transition relation of our airlock system using
  \texttt{Prop.}
  %
  In practice, this is done by enumerating the cases where the statement is
  true, and the remaining cases are considered false.

  \inputminted[frame=single]{coq}{Listings/Airlock2.v}

  This reads as follows: we distinguish between four classes of transitions, one
  per actions.
  %
  On the one hand, a transition which implies to open a door (case one and
  three) requires the both doors to be closed.
  %
  On the other hand, a transition which implies to close a door (case two and
  four) requires the door to be closed.

  The security property of our airlock system is a safety property: at any time,
  at least one door is close.
  %
  This property is a predicate on the system state, which we can define in {\sc
    Gallina} \emph{via} the \texttt{Prop} type.

  \inputminted[frame=single]{coq}{Listings/Airlock3.v}

  For the airlock to be secure, this means the \texttt{secure\_airlock}
  predicate must hold true before and after each transition.
  %
  We can express that in the form of a \emph{lemma}, that is a logical statement
  for each we exhibit a proof.
  %
  We write the proof with {\sc Ltac}, and trust Coq to refuse it in case it is
  incorrect.
  %
  The proof is written in terms of {\sc Ltac} tactics.
  %
  Each tactic corresponds to a reasoning rule.
  %
  For instance, the \texttt{right} tactic refers to the fact that given two
  statements $P$ and $Q$, then if $Q$ is true, then $P \vee Q$ is also
  true. Tactics updates the proof obligation (initially the lemma statement),
  and the goal of a Coq user is to bring back this proof obligation to something
  that has already been proven.

  \inputminted[frame=single]{coq}{Listings/Airlock4.v}

  This reads as follows: we prove that, given any ``secure state'', that is a
  airlock for which the \texttt{secure\_airlock} holds true, any action which
  implies a system's state update, the \texttt{secure\_airlock} also holds true
  for the system's state after the transition.
  %
  The proof is conducted by enumeration of possible actions
  (\texttt{induction}), and by definition of \texttt{Transition}
  (\texttt{inversion}).
\end{example}

\section{Design Methods Easing the Verification}
\label{sec:related:ease}

\subsection{Monads and Model Readability}

\begin{itemize}
\item State Monad
\item Hoare Monad
\end{itemize}

\subsection{Component-based Modelling}

Real world systems tend to be made of many components.
%
In such a case, the state of the system can be modelled as the scalar product of
its components' state.
%
This design choice has two important consequences:
%
\begin{inparaenum}[(1)]
%
\item it reduces modularity by explicitly coupling all the system components
  together, and
%
\item it encourages reducing the scope of the model to only take into account
  the hardware features which are directly required.
%
\end{inparaenum}

Process algebra is the study of distributed systems, by specifying separately
each of its components and verifying the properties of their composition.
%
Hardware architectures with several \ac{cpu} and active peripherals qualify
``distributed''.
%
Joonwon Choi \emph{et al.} have adapted ideas from process algebra to hardware
verification with Kami\,\cite{choi2017kami}.
%
We present here a simplified formalism, heavily inspired by their work.

An interface is modelled as a set of labels.
%
A component is modelled in terms of its state, the interface it exposes, the
interface it uses, and a transition relation, that is
$M \triangleq \langle S, I, O, \rightarrow \rangle$.
%
A transition of $M$ is denoted
%
\[ p \xrightarrow[a]{i} (v, q) \]
%
where $p \in S$ is the state of the component before the operation $i \in I$ is
called by an other component;
%
$a$ is a sequence of actions performed by the component in order to compute the
result of $i$;
%
$v$ is the result of this call, for the other component to use, and $q$ is the
modified state of the component.
%
Actions, in this context, are either local manipulations of the component's
state or calls of operations of the interface $O$.

A component $M$ can be composed with another component $M'$ to form a larger
component $M + M'$, for instance when the interface exposed by $M'$ is the
interface used by $M$.
%
Reasoning about $M + M'$ can be done using a two-steps methodology, where $M$ is
firstly verified under hypotheses about operations' results of $M'$, then these
hypotheses are proven to be true.

\section{A Tour of Large Verified Systems}

\subsection{Hardware Systems}

\paragraph{\ac{xom}.}
%
The \ac{xom} microprocessor architecture maintains separate so-called
\emph{compartments} for applications.
%
With mainstream microprocessor architectures, the system software is responsible
for both memory allocation and access control.
%
It relies on configurable \ac{cpu} mechanisms, such as a \ac{mmu}, to implement
the latter.
%
On the contrary, a \ac{xom} \ac{cpu} keeps track of each memory location owner,
thanks to a tagging mechanism, and prevents an application to access a memory
location it does not own.

In 2003, David Lie \emph{et al.} have modelled the \ac{xom} architecture using
the Mur$\varphi$ model checker.
%
This model follows the principle of a \ac{lts}: a set of states, a set of
labelled transitions (called \emph{rules} in the context of this work), and a
set of transitions.
%
There are two kinds of rules: the \ac{xom} instructions set, and some additional
capabilities given to the attacker.
%
As for the transitions set, it is defined in terms of pre and postconditions.
%
On the one hand, the precondition is parameterized by the current state and the
labelled event.
%
If the pre-condition is satisfied for a given hardware state and labelled
events, then it means that event can occur in this state.
%
On the other hand, the post condition is parameterized by the initial state, the
labelled event and the resulting state.
%
It determines the consequences of that event, in terms of state update.

The security properties targeted by the \ac{xom} architecture are enforceable
security properties, and the authors rely on Mur$\varphi$ to perform the state
exploration of their model.
%
The main advantage of a model checker, in this context, is to be able to print
the trace it has found not to satisfy the targeted security property.
%
This trace describes an attack path, that is the execution steps to reproduce in
order to defeat the security enforcement.
%
Hence, the authors have been able to show with their model that the \ac{xom}
architecture was subject to several replay attacks, and they leveraged their
model to validate their countermeasures.
%
However, the state explosion problem obliges the authors to simplify their
model, in order to reduce the state combinatory.

\subsection{Software Systems}

\paragraph{sel4}

\paragraph{CertiKOS}

\paragraph{VirtCert}
%
Between 2011 and 2014, Gilles Barthe \emph{et al.} have worked on an idealized
model of a hypervisor.
%
This model is defined in terms of state, actions and a semantics of actions as
state-transformers.
%
That is, even if the authors are not using the terms \ac{lts}, their model
follows the same principles.
%
In their context, the state mixes information about both hardware components
(\ac{cpu} execution mode, registers, memory content, etc.) and software
components (list of guests, current active guest, memory mapping for the
hypervisor and the guests, etc.).
%
The actions are the hypercalls, exposed by the hypervisor for the guests to use.
%
The semantics of actions as state-transformers, that is the set of possible
transitions for the modelled system, is defined in terms of pre and
postconditions.

First, they have shown that their ideal hypervisor was
%
\begin{inparaenum}[(1)]
\item ensuring strong isolation between guests, and
%
\item eventually processing every request performed by the
  guests\,\cite{barthe2011virtcert1}.
\end{inparaenum}
%
Then, they have incorporated the \ac{cpu} cache to their
model\,\cite{barthe2012virtcert2}.
%
Cache-based attacks, where attackers are able to infer information they should
not have access to by leveraging their knowledge of micro-architectural
implementation specificity, are a very important threat to virtualization
platforms.
%
The authors have shown their ideal hypervisor could prevent such attack, at the
cost of flushing the cache before each context switch.
%
They have taken their approach a step further, by focusing on constant-time
cryptography\,\cite{barthe2014virtcert3}.

The scope of this hypervisor model is large, and cover many security aspects
primordial for virtualization platforms.
%
Moreover, there have been an important specification and formalization effort
required to take into consideration the different security properties.
%
Some of them, like constant-time cryptography implementations, are not
enforceable security properties.
%
Indeed, it is not possible, only with the trace of one execution, to know
whether a given implemantion is constant time.
%
It is required to consider all the possible traces.

\subsection{Perspectives on HSE Mechanisms Verification}

%\begin{table}
%  \centerline{%
%    \begin{tabular}{lcp{7cm}}
%      \multicolumn{1}{c}{\bf Target}
%      & \bf Approach
%      & \multicolumn{1}{c}{\bf Hypotheses} \\
%      \hline
%      \hline
%      \ac{xom}
%      & Model Checking
%      & \\
%      \hline
%      VirtCert
%      & Theorem Proving
%      & \begin{asparaitem}
%      \item Simplified hardware model
%      \end{asparaitem} \\
%    \end{tabular}}
%\end{table}

The main characteristic of \ac{hse} mechanisms is that they are the result of
both hardware and software components, in presence of a larger system with
untrusted actors.
%
Hardware and software components are not of the same nature.
%
Hardware components are physical devices which accept inputs and compute
outputs.
%
Software components are pieces of data scattered into memory spaces, whose
semantics are determined by the processor unit that executes them.
%
While both hardware and software components have been subject to formal
verification in the past, these verification works tend to rely on different
representations.
%
Florian Lugou \emph{et al.} explain this in depth while they introduce SMASHUP,
a toolchain for unified verification of hardware-software
co-designs\,\cite{lugou2017smashup}.
%
This is why we have decided to focus on the specification level in the context
of this thesis.
%
Refining our potential conclusions up to a concrete implementation is necessary
in the long term.
%
Fortunately, it is also a very active field, with impressive flagship projects
such as DeepSpec\,\cite{appel2017deepspec}.

\section{Conclusion}